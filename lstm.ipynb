{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjanunicode/deeplearning_models_bhu/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "import io\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "data = pd.read_csv(io.BytesIO(uploaded['T.csv']))\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "4QCTZ3Idj-IC",
        "outputId": "6d446f89-0f5c-4276-8274-a45278bba7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d8e37d6-1505-4087-b1b2-9f68910d929a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d8e37d6-1505-4087-b1b2-9f68910d929a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving T.csv to T.csv\n",
            "            Date    Symbol Series  Prev Close     Open     High      Low  \\\n",
            "0     04-01-2010      BPCL     EQ      635.55   635.55   657.60   632.00   \n",
            "1     05-01-2010      BPCL     EQ      650.75   656.45   656.45   639.20   \n",
            "2     06-01-2010      BPCL     EQ      640.95   642.00   648.40   628.10   \n",
            "3     07-01-2010      BPCL     EQ      631.30   631.00   637.35   614.10   \n",
            "4     08-01-2010      BPCL     EQ      619.05   619.00   635.95   619.00   \n",
            "...          ...       ...    ...         ...      ...      ...      ...   \n",
            "9313  23-03-2016  RELIANCE     EQ     1047.85  1046.90  1046.90  1022.40   \n",
            "9314  28-03-2016  RELIANCE     EQ     1029.20  1030.00  1031.70  1015.65   \n",
            "9315  29-03-2016  RELIANCE     EQ     1020.40  1023.80  1042.00  1020.20   \n",
            "9316  30-03-2016  RELIANCE     EQ     1036.25  1042.70  1051.75  1042.70   \n",
            "9317  31-03-2016  RELIANCE     EQ     1047.05  1041.00  1056.00  1032.70   \n",
            "\n",
            "         Last    Close     VWAP   Volume      Turnover  Unnamed: 12  \n",
            "0      652.00   650.75   649.60   714541  4.640000e+13          NaN  \n",
            "1      640.20   640.95   642.94   804812  5.170000e+13          NaN  \n",
            "2      629.70   631.30   634.25   971566  6.160000e+13          NaN  \n",
            "3      620.00   619.05   623.40   922792  5.750000e+13          NaN  \n",
            "4      628.90   629.55   630.41   572148  3.610000e+13          NaN  \n",
            "...       ...      ...      ...      ...           ...          ...  \n",
            "9313  1027.25  1029.20  1031.48  3841654  3.960000e+14          NaN  \n",
            "9314  1022.50  1020.40  1022.77  4130553  4.220000e+14          NaN  \n",
            "9315  1034.00  1036.25  1034.32  3627901  3.750000e+14          NaN  \n",
            "9316  1047.40  1047.05  1047.52  3303780  3.460000e+14          NaN  \n",
            "9317  1045.00  1045.20  1045.52  7047985  7.370000e+14          NaN  \n",
            "\n",
            "[9318 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu4M-uxUjDyO",
        "outputId": "adada5c7-5b22-4d0c-c10f-8c300fd70432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date Symbol    Open    High    Low   Close  Volume\n",
            "0  04-01-2010   BPCL  635.55  657.60  632.0  650.75  714541\n",
            "1  05-01-2010   BPCL  656.45  656.45  639.2  640.95  804812\n",
            "2  06-01-2010   BPCL  642.00  648.40  628.1  631.30  971566\n",
            "3  07-01-2010   BPCL  631.00  637.35  614.1  619.05  922792\n",
            "4  08-01-2010   BPCL  619.00  635.95  619.0  629.55  572148\n",
            "        Name\n",
            "0       BPCL\n",
            "1       GAIL\n",
            "2       NTPC\n",
            "3       ONGC\n",
            "4  POWERGRID\n",
            "5   RELIANCE\n",
            "[650.75, 640.95, 631.3, 619.05, 629.55, 627.9, 627.55, 629.8, 625.8, 617.35, 618.4, 605.6, 577.5, 573.25, 571.75, 569.7, 554.45, 542.2, 540.85, 584.05, 565.65, 580.05, 583.0, 568.15, 578.55, 573.1, 560.55, 572.75, 566.25, 557.6, 569.9, 564.15, 591.2, 583.1, 592.25, 579.25, 581.95, 566.95, 563.95, 540.95, 541.9, 534.9, 540.25, 541.9, 536.45, 537.15, 530.05, 526.75, 528.2, 545.25, 538.0, 526.7, 522.6, 517.5, 511.6, 505.15, 510.1, 526.9, 522.1, 518.05, 511.85, 508.55, 506.9, 505.4, 508.85, 505.9, 500.5, 507.8, 508.1, 499.6, 499.25, 501.05, 499.65, 499.2, 498.75, 502.15, 503.55, 505.1, 491.3, 517.5, 519.2, 515.8, 526.55, 539.3, 549.9, 539.7, 541.65, 540.45, 537.25, 549.0, 542.65, 544.3, 544.3, 557.7, 558.8, 559.75, 549.25, 555.25, 572.75, 563.6, 581.15, 576.65, 575.45, 579.9, 579.8, 563.5, 543.75, 552.75, 551.0, 541.0, 563.15, 530.6, 534.3, 535.4, 521.7, 526.55, 550.15, 558.8, 550.75, 620.7, 642.5, 635.25, 662.75, 660.3, 666.75, 658.75, 663.55, 659.1, 698.5, 710.8, 689.85, 700.8, 700.55, 656.3, 661.05, 655.5, 641.05, 646.1, 641.15, 632.1, 624.3, 630.15, 643.6, 638.9, 641.55, 643.55, 653.8, 650.2, 651.15, 650.1, 647.3, 646.85, 660.35, 674.75, 670.85, 658.35, 673.2, 680.25, 676.55, 698.7, 776.05, 760.15, 751.75, 763.65, 776.4, 770.8, 761.25, 778.7, 779.0, 766.9, 763.25, 758.25, 756.95, 750.8, 752.3, 770.05, 767.0, 755.25, 765.9, 802.15, 793.95, 787.6, 783.0, 789.3, 788.4, 781.75, 765.15, 749.55, 755.6, 756.75, 764.9, 759.5, 752.95, 747.05, 744.1, 744.15, 760.9, 723.15, 695.15, 699.2, 707.75, 714.25, 720.95, 719.5, 723.3, 709.5, 702.1, 720.2, 730.05, 754.8, 772.75, 765.6, 760.35, 762.45, 741.05, 744.05, 755.75, 747.1, 730.45, 737.3, 725.45, 738.95, 712.65, 717.1, 705.4, 700.55, 672.7, 667.7, 680.3, 675.05, 697.95, 685.8, 673.85, 668.5, 661.05, 677.7, 673.3, 680.7, 678.8, 675.8, 703.35, 699.5, 691.4, 683.4, 696.0, 688.35, 672.45, 666.4, 664.0, 669.1, 659.2, 658.4, 659.95, 653.55, 632.55, 618.75, 616.95, 605.35, 609.8, 618.9, 597.9, 593.85, 585.6, 589.4, 584.3, 575.5, 590.9, 601.2, 630.95, 636.7, 632.45, 613.0, 580.5, 593.65, 595.6, 588.45, 594.9, 599.4, 584.95, 585.55, 582.95, 603.65, 603.9, 603.5, 615.4, 591.75, 589.5, 570.25, 562.75, 539.65, 539.1, 553.45, 584.35, 570.8, 580.0, 556.6, 561.8, 551.5, 543.35, 545.85, 570.45, 570.7, 576.55, 573.95, 558.85, 563.85, 565.7, 572.3, 577.25, 592.4, 588.6, 612.45, 613.55, 611.85, 605.9, 598.2, 595.3, 603.0, 613.8, 602.1, 587.5, 601.2, 599.4, 589.15, 605.1, 617.65, 619.75, 618.45, 630.0, 631.45, 634.65, 629.55, 633.4, 629.35, 654.5, 643.35, 666.05, 655.0, 651.55, 658.9, 660.6, 660.4, 652.35, 645.2, 618.45, 617.75, 625.7, 629.3, 619.9, 603.75, 616.2, 628.15, 627.25, 633.0, 632.55, 628.65, 621.2, 612.05, 614.0, 624.75, 612.95, 605.9, 615.0, 617.15, 619.05, 629.7, 630.9, 627.65, 618.25, 615.7, 617.2, 634.7, 663.95, 651.75, 643.9, 650.25, 645.15, 646.95, 659.0, 651.05, 664.05, 669.25, 667.1, 668.15, 668.75, 666.9, 672.05, 672.8, 672.05, 667.5, 660.7, 668.95, 671.3, 662.35, 658.7, 654.4, 657.85, 655.3, 657.95, 665.75, 688.6, 702.3, 702.95, 692.1, 693.7, 689.9, 690.85, 687.85, 685.5, 679.3, 681.75, 691.0, 688.15, 685.7, 688.9, 670.85, 678.65, 674.6, 682.55, 664.9, 671.05, 672.2, 673.95, 662.6, 659.55, 656.7, 660.4, 673.4, 659.6, 663.65, 665.0, 663.35, 659.35, 657.35, 658.7, 657.95, 639.85, 656.6, 647.8, 679.55, 674.0, 669.7, 669.25, 672.35, 675.2, 671.9, 654.5, 655.45, 632.45, 641.95, 648.15, 642.35, 644.45, 644.1, 657.4, 657.15, 637.1, 622.6, 629.0, 628.85, 630.5, 624.35, 603.45, 569.65, 558.75, 550.0, 538.45, 515.95, 508.5, 520.95, 505.15, 521.9, 500.45, 502.95, 518.65, 535.5, 528.0, 537.95, 526.15, 557.5, 555.6, 549.5, 550.25, 554.5, 543.6, 541.0, 514.65, 529.65, 527.8, 524.8, 500.6, 515.0, 521.4, 506.5, 500.9, 503.7, 495.0, 478.65, 477.8, 480.5, 472.75, 470.7, 459.9, 476.5, 476.15, 469.05, 482.7, 507.5, 512.45, 508.65, 505.3, 525.35, 535.8, 542.2, 559.9, 545.45, 556.35, 579.6, 577.3, 564.4, 572.15, 560.0, 575.7, 575.85, 573.7, 567.75, 578.1, 598.75, 598.2, 589.7, 598.3, 604.0, 622.85, 619.2, 614.0, 623.65, 656.35, 656.35, 659.65, 652.7, 662.25, 658.85, 663.75, 674.7, 666.1, 668.75, 659.35, 664.3, 663.0, 667.25, 681.65, 667.4, 664.25, 665.55, 685.3, 695.4, 679.75, 680.85, 665.4, 667.35, 667.15, 687.85, 700.25, 687.35, 705.85, 699.15, 680.5, 667.5, 670.5, 679.9, 677.85, 678.45, 686.85, 698.45, 694.8, 696.25, 674.65, 687.6, 686.25, 661.0, 662.8, 666.8, 670.65, 657.55, 661.65, 652.4, 686.15, 680.35, 688.9, 704.25, 700.3, 713.85, 728.75, 752.1, 735.85, 724.9, 725.8, 728.1, 722.2, 726.3, 730.95, 716.15, 701.1, 690.9, 697.65, 694.5, 716.5, 704.3, 705.5, 700.0, 702.3, 711.25, 732.1, 732.4, 739.35, 742.55, 738.5, 751.2, 759.4, 763.55, 763.15, 748.4, 749.45, 749.8, 751.15, 743.95, 747.15, 759.95, 753.7, 758.3, 763.7, 762.9, 768.9, 776.8, 785.45, 386.55, 388.6, 376.7, 376.75, 388.3, 383.1, 373.85, 372.1, 366.25, 360.25, 355.5, 366.6, 363.3, 360.15, 354.55, 353.0, 344.4, 341.35, 351.65, 340.3, 352.55, 350.65, 346.45, 343.85, 340.6, 338.75, 348.9, 351.15, 354.55, 354.0, 354.25, 352.15, 348.0, 334.85, 340.9, 346.95, 345.45, 347.3, 345.1, 344.25, 342.5, 349.2, 347.55, 354.7, 350.2, 338.75, 337.45, 345.3, 347.3, 344.85, 349.35, 351.85, 346.85, 348.7, 351.5, 348.05, 359.85, 356.5, 355.35, 357.4, 351.7, 352.8, 351.15, 349.85, 345.5, 344.45, 347.05, 339.65, 342.4, 348.55, 350.5, 348.0, 342.6, 338.1, 337.55, 337.9, 336.05, 339.8, 341.45, 339.85, 339.15, 338.4, 333.3, 333.3, 330.7, 329.05, 328.05, 326.4, 328.65, 325.95, 321.45, 315.45, 328.35, 336.6, 347.5, 343.9, 349.7, 358.7, 360.6, 358.2, 356.7, 359.65, 353.85, 351.45, 350.95, 344.9, 350.25, 357.65, 349.9, 343.8, 346.95, 348.75, 344.05, 352.3, 356.35, 356.3, 366.45, 366.25, 374.0, 385.5, 380.75, 385.7, 380.85, 370.75, 373.15, 385.2, 382.2, 396.15, 434.95, 440.9, 433.8, 424.55, 415.9, 419.95, 415.75, 409.5, 409.35, 410.45, 422.65, 418.8, 411.75, 418.35, 410.85, 401.4, 402.55, 410.3, 401.45, 387.35, 379.5, 379.3, 381.75, 392.9, 390.25, 389.25, 382.85, 376.05, 380.05, 371.95, 385.5, 387.25, 388.4, 384.25, 390.65, 405.4, 399.85, 396.75, 390.55, 397.25, 400.05, 392.1, 382.25, 380.3, 368.85, 366.05, 372.15, 374.7, 377.7, 377.0, 377.7, 375.0, 369.55, 376.5, 380.45, 372.45, 369.5, 366.3, 374.35, 394.8, 406.55, 401.35, 405.35, 414.7, 413.25, 416.75, 413.05, 414.3, 414.25, 416.85, 403.3, 408.45, 406.7, 409.35, 409.8, 415.05, 418.35, 409.6, 409.75, 415.9, 421.9, 418.25, 409.75, 402.35, 389.65, 379.9, 382.45, 377.45, 380.6, 378.75, 383.7, 376.85, 367.85, 371.35, 373.65, 373.0, 376.2, 366.3, 359.05, 362.7, 357.3, 368.9, 366.9, 364.85, 364.75, 351.35, 348.35, 338.95, 344.95, 348.55, 345.65, 366.95, 374.35, 371.55, 357.4, 366.6, 365.4, 345.7, 345.6, 335.05, 339.35, 343.95, 340.5, 347.0, 342.2, 343.55, 346.65, 346.25, 352.75, 348.05, 350.15, 350.35, 340.4, 311.3, 323.75, 304.55, 289.85, 281.85, 261.45, 273.65, 276.0, 283.85, 286.05, 304.5, 290.2, 288.4, 297.65, 284.45, 295.35, 301.1, 305.3, 290.1, 266.95, 270.1, 273.0, 276.4, 266.95, 270.85, 287.3, 299.65, 298.45, 312.4, 303.8, 305.15, 317.85, 313.7, 315.75, 336.4, 337.95, 323.4, 315.35, 305.5, 309.2, 328.6, 331.25, 326.35, 335.3, 341.45, 352.85, 341.7, 340.25, 343.05, 338.95, 342.65, 343.1, 348.05, 362.2, 362.8, 361.0, 356.1, 356.95, 348.8, 346.8, 351.25, 361.65, 359.55, 355.8, 364.5, 372.9, 369.85, 357.55, 351.9, 339.1, 339.75, 333.2, 344.15, 352.5, 345.5, 335.55, 331.1, 332.85, 347.4, 324.85, 330.2, 333.8, 340.0, 345.4, 350.05, 350.7, 358.35, 364.3, 365.4, 357.7, 350.1, 350.0, 342.05, 339.0, 342.05, 346.15, 338.2, 344.0, 347.1, 348.0, 354.45, 351.65, 348.35, 347.9, 349.2, 339.45, 329.3, 329.8, 323.55, 317.55, 325.25, 326.3, 331.5, 328.2, 327.3, 330.35, 340.1, 342.55, 337.3, 343.95, 347.3, 343.15, 337.65, 342.2, 354.9, 352.5, 362.7, 356.45, 355.3, 354.6, 352.85, 352.15, 354.8, 359.65, 364.75, 355.0, 355.8, 357.5, 355.0, 351.4, 346.8, 361.25, 364.0, 371.85, 377.45, 378.0, 376.65, 380.15, 387.45, 401.1, 417.95, 418.8, 414.65, 412.9, 443.8, 444.75, 449.3, 448.2, 431.2, 428.1, 430.15, 434.0, 445.55, 445.15, 454.55, 458.25, 460.05, 446.9, 448.95, 439.6, 443.6, 438.1, 440.05, 451.85, 444.35, 445.7, 436.35, 449.8, 463.85, 482.8, 475.1, 460.65, 456.45, 460.45, 454.9, 462.6, 467.65, 467.5, 472.35, 462.45, 485.8, 502.45, 509.4, 518.65, 516.35, 552.1, 575.9, 558.75, 560.3, 559.65, 556.2, 557.8, 554.1, 542.3, 536.1, 521.9, 555.55, 573.65, 575.7, 619.45, 628.1, 615.05, 614.05, 607.05, 599.05, 569.75, 590.95, 610.25, 591.7, 564.45, 552.55, 556.65, 582.75, 581.3, 567.85, 569.7, 600.4, 589.6, 601.8, 599.6, 599.75, 593.25, 571.35, 582.85, 580.75, 561.2, 569.05, 582.05, 590.8, 591.1, 583.0, 581.4, 582.75, 600.25, 603.0, 591.55, 574.75, 573.4, 580.15, 577.35, 600.4, 592.85, 581.3, 584.95, 566.55, 573.45, 590.7, 603.65, 613.35, 648.5, 674.05, 667.0, 685.4, 672.0, 664.7, 664.7, 677.75, 694.3, 704.85, 703.65, 704.55, 700.75, 688.7, 700.05, 692.65, 676.9, 681.4, 683.15, 683.35, 650.0, 638.95, 662.3, 657.25, 670.35, 650.95, 642.25, 620.3, 639.45, 637.3, 655.4, 652.85, 650.55, 674.7, 671.7, 670.25, 671.75, 656.3, 666.25, 666.75, 696.85, 687.8, 688.75, 691.1, 686.15, 701.85, 704.65, 702.05, 724.75, 724.05, 740.3, 759.15, 766.0, 766.9, 765.3, 730.45, 734.85, 726.05, 719.9, 721.55, 719.8, 724.45, 719.4, 709.9, 707.6, 720.55, 746.25, 744.35, 712.55, 710.85, 726.55, 710.05, 689.75, 673.35, 668.95, 663.15, 667.6, 636.35, 647.45, 647.8, 653.3, 653.75, 655.25, 650.8, 639.7, 645.7, 647.1, 648.45, 646.0, 654.2, 644.4, 645.65, 639.15, 644.95, 677.6, 679.35, 676.4, 669.55, 651.7, 662.95, 659.3, 645.2, 647.65, 654.15, 657.05, 674.75, 692.05, 706.25, 733.85, 748.85, 748.65, 727.4, 721.0, 750.0, 730.95, 711.8, 698.3, 712.9, 705.85, 724.9, 725.85, 729.65, 742.2, 741.8, 739.65, 757.3, 751.85, 745.8, 744.25, 746.2, 779.5, 780.8, 770.5, 776.35, 764.0, 764.2, 750.85, 749.5, 731.4, 736.65, 750.65, 772.45, 753.4, 727.05, 715.4, 740.8, 748.7, 762.3, 749.6, 769.3, 810.8, 804.2, 802.1, 795.7, 819.65, 811.85, 820.05, 818.4, 802.4, 800.9, 797.85, 803.5, 799.15, 805.05, 802.55, 780.5, 732.3, 765.15, 755.95, 764.75, 783.7, 777.3, 748.2, 729.95, 748.05, 750.8, 725.9, 735.85, 756.7, 767.05, 792.8, 784.3, 786.95, 776.95, 782.3, 791.8, 781.75, 796.9, 817.7, 850.15, 860.75, 846.75, 839.2, 866.7, 855.7, 843.05, 849.4, 843.9, 842.4, 837.5, 833.5, 820.5, 810.45, 833.8, 835.65, 846.15, 860.85, 848.4, 858.35, 862.05, 871.85, 879.35, 887.95, 900.0, 902.85, 930.85, 932.35, 916.1, 866.65, 881.75, 909.3, 926.95, 917.65, 938.0, 939.65, 963.4, 952.2, 972.05, 970.3, 965.55, 954.45, 944.95, 949.3, 947.2, 926.75, 908.8, 933.55, 930.6, 938.6, 955.5, 946.7, 925.25, 870.0, 885.25, 873.75, 865.15, 896.45, 887.1, 902.05, 889.65, 818.6, 862.75, 837.65, 880.2, 860.95, 883.2, 863.35, 849.1, 837.95, 850.6, 830.2, 821.25, 820.25, 839.7, 850.55, 880.4, 879.6, 862.35, 869.55, 871.25, 861.4, 876.75, 880.9, 883.5, 847.8, 850.7, 855.1, 860.7, 875.65, 863.55, 854.95, 848.5, 849.05, 863.5, 862.3, 893.1, 911.25, 918.5, 912.45, 900.6, 910.2, 903.75, 914.65, 909.4, 879.1, 871.1, 871.8, 876.7, 886.0, 889.0, 896.1, 902.9, 876.85, 889.45, 906.05, 898.65, 876.25, 884.0, 907.4, 916.1, 905.7, 907.4, 904.95, 901.35, 901.75, 932.7, 932.15, 919.95, 906.5, 910.75, 912.0, 882.25, 904.15, 892.9, 890.05, 889.65, 903.55, 903.95, 900.75, 897.5, 902.7, 900.8, 904.1, 903.3, 902.6, 891.7, 892.3, 896.05, 891.45, 902.65, 912.3, 884.25, 903.55, 888.05, 890.15, 895.9, 909.4, 932.55, 896.6, 895.8, 885.5, 875.55, 883.25, 877.9, 870.6, 868.9, 893.15, 880.15, 850.3, 829.55, 831.75, 843.0, 825.2, 824.9, 837.55, 817.75, 772.8, 779.75, 751.0, 780.95, 789.75, 765.85, 759.4, 736.05, 768.9, 764.15, 764.7, 769.25, 793.7, 786.6, 789.95, 815.2, 820.9, 799.35, 794.15, 803.65, 804.75, 810.9, 818.4, 864.85, 856.9, 870.05, 870.7, 868.5, 873.6, 874.0, 901.15, 904.3]\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 14s 238ms/step - loss: 0.0159 - val_loss: 0.0155\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0046 - val_loss: 0.0029\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 5s 172ms/step - loss: 0.0041 - val_loss: 0.0025\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 6s 196ms/step - loss: 0.0038 - val_loss: 0.0030\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0034 - val_loss: 0.0048\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 6s 207ms/step - loss: 0.0034 - val_loss: 0.0024\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0029 - val_loss: 0.0034\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 6s 205ms/step - loss: 0.0026 - val_loss: 0.0018\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0025 - val_loss: 0.0065\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 6s 183ms/step - loss: 0.0022 - val_loss: 0.0128\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 6s 183ms/step - loss: 0.0022 - val_loss: 0.0037\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 5s 163ms/step - loss: 0.0025 - val_loss: 0.0015\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 6s 205ms/step - loss: 0.0020 - val_loss: 0.0058\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0016 - val_loss: 0.0035\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 6s 196ms/step - loss: 0.0016 - val_loss: 0.0079\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 5s 172ms/step - loss: 0.0014 - val_loss: 0.0024\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0015 - val_loss: 0.0040\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0014 - val_loss: 0.0019\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0012 - val_loss: 0.0060\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 7s 225ms/step - loss: 0.0015 - val_loss: 0.0012\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0012 - val_loss: 0.0069\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 6s 183ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 6s 186ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0011 - val_loss: 0.0026\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0011 - val_loss: 0.0010\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 9.5930e-04 - val_loss: 8.4341e-04\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0010 - val_loss: 0.0010\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 9.9969e-04 - val_loss: 0.0023\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 8.6997e-04 - val_loss: 0.0015\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 6s 195ms/step - loss: 8.1883e-04 - val_loss: 0.0010\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 6s 182ms/step - loss: 8.6482e-04 - val_loss: 8.0487e-04\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 9.1797e-04 - val_loss: 0.0024\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 8.8329e-04 - val_loss: 7.9090e-04\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 7.8847e-04 - val_loss: 0.0017\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 8.0969e-04 - val_loss: 0.0011\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 8.8401e-04 - val_loss: 0.0060\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 6s 201ms/step - loss: 8.9611e-04 - val_loss: 6.8206e-04\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 5s 173ms/step - loss: 7.6210e-04 - val_loss: 7.9928e-04\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 7.7820e-04 - val_loss: 0.0022\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 6s 201ms/step - loss: 8.6911e-04 - val_loss: 6.2292e-04\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 7.0463e-04 - val_loss: 6.7526e-04\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 6s 207ms/step - loss: 6.4515e-04 - val_loss: 8.2284e-04\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 7.4106e-04 - val_loss: 0.0015\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 6s 207ms/step - loss: 7.2443e-04 - val_loss: 9.6726e-04\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 7.4260e-04 - val_loss: 0.0016\n",
            "31/31 [==============================] - 2s 42ms/step\n",
            "12/12 [==============================] - 1s 64ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d31c2acb9fe7>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  final_df = final_df.append({'Company': x, 'close price':ds[-1:], 'predicted price':output},ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[415.05, 418.7, 415.9, 426.35, 428.75, 432.4, 427.6, 425.5, 429.95, 425.5, 438.5, 426.05, 421.4, 411.25, 399.75, 400.05, 390.9, 396.9, 394.05, 412.6, 398.4, 405.7, 418.7, 415.9, 419.25, 410.1, 408.7, 408.0, 412.25, 406.85, 410.1, 410.9, 404.1, 404.5, 404.4, 408.7, 406.15, 399.15, 398.8, 392.6, 400.1, 400.1, 408.85, 411.2, 407.6, 407.4, 417.25, 419.2, 411.15, 414.05, 420.95, 418.45, 414.1, 407.65, 407.85, 401.7, 401.55, 400.6, 400.4, 410.6, 410.5, 417.25, 417.05, 431.65, 426.8, 424.6, 411.2, 410.15, 402.95, 409.65, 404.35, 425.45, 426.55, 422.65, 422.1, 423.1, 424.75, 421.45, 416.15, 429.1, 428.05, 426.75, 419.65, 426.55, 419.85, 427.75, 425.2, 431.7, 428.85, 427.75, 433.0, 440.95, 432.2, 441.85, 453.75, 451.75, 446.4, 448.05, 452.15, 448.3, 453.65, 449.95, 454.35, 461.65, 460.75, 452.9, 449.95, 455.65, 456.1, 459.9, 461.9, 462.2, 463.6, 475.15, 472.45, 473.35, 470.55, 476.0, 476.75, 484.15, 473.8, 472.95, 467.3, 466.25, 461.8, 465.0, 465.25, 457.75, 469.6, 471.8, 465.05, 466.9, 459.2, 448.05, 440.6, 446.7, 448.45, 459.35, 474.45, 464.85, 456.7, 449.4, 445.2, 452.75, 438.5, 446.9, 444.3, 441.0, 430.1, 437.55, 437.95, 443.1, 448.35, 448.95, 453.7, 452.15, 451.15, 461.5, 461.05, 468.55, 465.5, 455.4, 448.2, 446.55, 460.15, 459.35, 462.05, 470.05, 459.7, 461.6, 468.1, 465.6, 466.9, 461.6, 470.8, 464.9, 483.45, 470.35, 480.35, 480.0, 479.65, 476.2, 478.1, 479.6, 479.85, 477.75, 474.65, 477.55, 484.85, 488.55, 491.8, 488.65, 483.7, 490.15, 495.6, 499.75, 508.05, 503.65, 499.85, 501.6, 509.5, 496.05, 501.85, 504.45, 505.1, 496.75, 493.05, 490.05, 491.55, 492.25, 486.35, 486.6, 494.65, 495.3, 495.85, 497.7, 500.15, 490.45, 485.6, 494.15, 490.1, 489.15, 490.4, 495.15, 489.35, 485.35, 488.45, 487.5, 494.65, 491.5, 495.25, 491.7, 485.8, 493.45, 501.95, 507.35, 496.9, 492.6, 500.95, 504.6, 502.5, 500.0, 507.75, 508.0, 509.25, 508.35, 513.75, 509.65, 506.0, 506.8, 510.7, 512.65, 515.05, 520.6, 528.8, 527.15, 524.45, 502.75, 500.65, 508.9, 498.6, 486.75, 481.85, 478.25, 467.05, 453.25, 453.0, 460.2, 474.55, 459.55, 454.6, 470.95, 462.25, 461.15, 463.9, 453.05, 445.15, 441.55, 446.3, 452.05, 445.85, 456.0, 457.55, 458.55, 452.95, 442.85, 448.9, 442.2, 443.45, 437.75, 437.5, 426.55, 444.95, 455.25, 455.3, 443.85, 441.0, 441.15, 441.85, 442.4, 462.65, 457.75, 456.75, 452.35, 447.9, 450.45, 453.6, 466.5, 459.15, 458.6, 450.75, 453.85, 459.15, 464.0, 463.55, 457.1, 461.8, 475.0, 473.15, 471.15, 464.2, 475.1, 469.3, 463.85, 472.6, 478.8, 474.1, 479.45, 482.7, 475.9, 475.25, 475.45, 460.95, 445.75, 446.65, 450.0, 450.05, 456.6, 462.1, 460.95, 460.15, 459.15, 452.15, 433.3, 421.75, 428.15, 426.2, 435.25, 429.95, 429.75, 442.1, 434.7, 443.0, 444.8, 452.65, 447.7, 439.6, 432.15, 435.75, 443.5, 445.05, 445.15, 450.0, 451.35, 442.6, 436.65, 439.7, 430.7, 434.0, 437.7, 437.8, 448.3, 457.4, 446.35, 441.65, 441.2, 450.75, 460.5, 456.5, 459.55, 466.85, 462.85, 463.05, 457.4, 463.75, 456.7, 446.15, 455.0, 457.0, 456.0, 457.6, 466.5, 466.75, 461.25, 458.45, 459.05, 461.1, 464.65, 462.15, 463.05, 467.0, 455.8, 449.1, 437.85, 437.2, 440.35, 448.75, 433.85, 438.2, 438.2, 429.7, 415.25, 412.7, 411.65, 413.0, 411.25, 416.95, 410.6, 414.95, 417.7, 427.25, 426.95, 423.05, 414.15, 409.1, 408.1, 415.85, 434.2, 423.75, 433.6, 436.4, 431.15, 423.35, 421.9, 409.8, 412.9, 423.6, 425.85, 410.55, 418.2, 410.15, 405.4, 411.1, 421.6, 417.15, 419.7, 418.0, 415.35, 409.6, 413.8, 425.95, 421.95, 418.4, 425.7, 426.5, 428.15, 429.0, 422.9, 423.1, 419.15, 427.75, 423.85, 421.6, 416.85, 415.1, 418.6, 407.3, 391.5, 386.55, 381.3, 372.7, 374.0, 372.9, 386.9, 384.8, 388.8, 389.1, 391.65, 397.15, 398.45, 399.05, 402.25, 395.6, 392.55, 391.5, 390.0, 373.9, 371.55, 377.75, 376.2, 377.95, 388.3, 393.5, 391.15, 395.6, 395.35, 395.0, 382.1, 383.65, 386.75, 392.7, 394.35, 381.0, 380.85, 383.15, 380.65, 380.8, 380.3, 385.25, 373.45, 372.3, 370.45, 364.9, 368.9, 372.35, 369.45, 367.65, 366.15, 372.5, 368.8, 373.3, 376.7, 388.5, 391.45, 389.55, 379.1, 391.75, 389.15, 385.85, 388.1, 385.55, 396.95, 385.0, 380.7, 380.95, 375.15, 373.6, 374.9, 371.55, 374.2, 374.9, 373.75, 371.25, 371.6, 353.25, 351.5, 347.55, 349.15, 354.4, 370.2, 375.1, 374.8, 366.6, 363.6, 364.05, 369.45, 361.85, 369.9, 362.6, 363.5, 366.2, 367.9, 376.25, 377.15, 383.35, 371.1, 363.15, 355.6, 354.25, 359.05, 356.25, 355.85, 365.4, 367.4, 360.45, 360.85, 353.3, 347.15, 335.75, 323.65, 328.35, 330.4, 330.45, 329.35, 324.85, 320.95, 318.9, 323.75, 322.6, 322.45, 318.7, 318.65, 315.05, 310.2, 310.9, 317.8, 320.6, 313.9, 324.75, 324.6, 337.15, 327.75, 329.9, 325.2, 321.2, 330.25, 319.6, 321.95, 326.65, 320.65, 331.9, 336.5, 337.4, 337.5, 333.55, 333.95, 327.4, 336.15, 339.3, 339.95, 340.6, 343.65, 351.95, 354.7, 345.9, 352.1, 355.15, 361.85, 362.6, 361.05, 356.7, 351.25, 358.15, 356.85, 360.45, 363.3, 355.45, 355.7, 355.35, 356.7, 354.75, 339.1, 343.3, 344.65, 344.2, 345.25, 355.75, 353.85, 353.45, 351.85, 357.1, 365.55, 378.7, 366.55, 369.05, 367.4, 368.95, 375.1, 367.35, 360.75, 369.65, 367.15, 366.5, 365.9, 365.5, 366.05, 364.85, 356.5, 355.05, 349.6, 358.55, 356.05, 361.0, 364.65, 363.05, 367.4, 373.0, 367.35, 369.55, 374.55, 377.05, 388.0, 374.6, 385.35, 384.0, 383.7, 386.7, 382.85, 382.75, 382.6, 384.85, 391.4, 393.05, 391.45, 381.05, 375.5, 376.35, 379.95, 380.0, 378.4, 371.05, 368.2, 360.55, 358.8, 355.05, 361.2, 364.25, 361.45, 355.6, 348.9, 345.5, 358.8, 357.0, 363.8, 366.1, 358.6, 355.0, 352.15, 351.45, 354.05, 346.3, 342.55, 344.05, 346.5, 347.35, 339.9, 337.0, 339.65, 344.45, 353.35, 348.55, 352.3, 353.8, 353.8, 352.85, 354.35, 354.95, 347.8, 347.6, 346.45, 344.15, 346.95, 349.9, 351.75, 349.2, 352.15, 353.9, 347.55, 351.3, 356.75, 357.4, 364.75, 363.35, 371.15, 366.35, 364.55, 370.6, 367.45, 364.45, 370.9, 372.25, 372.9, 376.4, 383.65, 388.0, 369.95, 365.95, 349.5, 353.55, 358.1, 350.55, 339.15, 342.25, 343.2, 339.8, 344.9, 343.8, 335.45, 335.6, 335.25, 333.4, 328.05, 334.65, 338.75, 340.15, 334.45, 338.0, 338.15, 341.0, 343.2, 339.75, 333.55, 332.55, 332.05, 330.05, 337.0, 334.0, 331.5, 341.6, 345.9, 346.35, 339.6, 332.55, 323.3, 313.7, 323.45, 318.65, 318.85, 319.1, 313.8, 303.75, 318.25, 321.35, 321.75, 314.25, 312.1, 316.45, 320.4, 313.8, 315.55, 315.3, 316.0, 317.6, 320.95, 322.3, 333.05, 335.45, 330.05, 344.4, 351.05, 350.9, 351.15, 346.8, 335.15, 339.85, 342.7, 342.5, 339.8, 342.3, 341.1, 331.95, 337.55, 339.9, 338.2, 336.9, 336.55, 336.45, 329.85, 324.3, 321.95, 324.5, 326.65, 321.6, 320.45, 306.05, 311.45, 311.15, 314.95, 310.45, 307.05, 303.75, 305.95, 304.95, 296.55, 302.65, 299.7, 294.35, 297.85, 291.4, 289.65, 285.25, 286.1, 296.45, 299.05, 313.3, 324.9, 332.55, 329.25, 333.75, 326.95, 317.95, 317.35, 314.65, 320.5, 322.7, 323.05, 323.95, 321.7, 324.5, 332.7, 330.05, 332.25, 330.35, 329.35, 319.1, 317.85, 306.15, 301.2, 305.05, 304.6, 305.15, 302.9, 304.7, 303.05, 300.15, 308.6, 319.2, 298.3, 299.35, 296.95, 298.05, 305.6, 304.8, 295.05, 293.05, 281.95, 288.8, 295.45, 302.15, 300.65, 302.4, 297.05, 301.1, 302.25, 303.35, 309.1, 313.8, 311.75, 316.95, 317.2, 328.15, 340.0, 335.05, 336.95, 334.5, 326.55, 326.3, 327.8, 324.8, 332.75, 331.6, 335.0, 333.8, 334.5, 331.5, 336.3, 330.1, 325.7, 325.35, 327.2, 327.8, 333.85, 347.0, 352.0, 343.6, 344.85, 344.75, 345.0, 353.6, 351.0, 350.85, 345.8, 345.15, 340.6, 339.5, 336.35, 331.7, 322.75, 320.15, 321.25, 327.4, 325.15, 321.55, 328.05, 332.8, 328.9, 328.95, 331.65, 338.45, 333.6, 342.7, 339.7, 344.75, 346.55, 352.9, 349.95, 344.35, 345.8, 338.05, 334.35, 334.5, 342.85, 336.55, 339.65, 341.4, 338.25, 340.4, 340.25, 341.9, 342.3, 343.45, 338.75, 337.4, 336.2, 338.0, 345.95, 347.5, 345.3, 344.75, 345.1, 348.9, 351.2, 351.6, 349.45, 344.0, 343.2, 350.95, 350.1, 344.8, 345.1, 346.0, 350.3, 358.3, 365.2, 354.65, 360.5, 357.15, 356.2, 350.3, 348.0, 356.85, 351.15, 360.75, 363.2, 357.8, 360.2, 354.75, 357.95, 363.85, 356.7, 368.7, 370.75, 369.25, 380.55, 375.0, 383.75, 387.75, 381.1, 370.5, 370.75, 369.1, 372.3, 369.55, 360.0, 350.75, 353.05, 352.4, 369.75, 367.55, 373.45, 379.1, 381.2, 376.05, 374.95, 377.6, 367.8, 362.2, 364.7, 373.45, 377.3, 371.5, 368.05, 368.0, 373.85, 373.45, 378.95, 383.85, 379.65, 370.05, 368.2, 370.1, 371.2, 372.0, 369.8, 366.15, 365.65, 371.0, 383.8, 393.45, 395.55, 404.35, 409.7, 429.85, 421.95, 413.85, 425.6, 425.4, 410.15, 379.25, 371.45, 371.55, 377.8, 390.75, 381.6, 381.05, 391.05, 422.1, 431.7, 428.6, 431.65, 432.05, 415.45, 433.3, 444.95, 452.65, 443.6, 439.1, 439.15, 459.5, 467.5, 460.0, 456.9, 463.05, 460.8, 458.85, 459.1, 465.0, 461.05, 455.5, 459.15, 463.05, 455.5, 454.45, 460.2, 457.2, 453.3, 445.05, 437.8, 440.7, 438.45, 430.15, 427.8, 424.35, 433.0, 435.95, 423.65, 424.25, 423.95, 421.55, 419.05, 407.6, 390.4, 413.45, 411.8, 425.7, 431.75, 432.15, 427.4, 431.5, 433.05, 427.4, 434.35, 435.7, 443.4, 460.1, 464.95, 455.35, 448.7, 451.9, 454.8, 458.7, 455.95, 459.7, 462.6, 458.35, 448.55, 445.8, 451.55, 449.3, 454.05, 443.35, 442.65, 451.85, 443.45, 452.9, 449.45, 438.9, 443.45, 441.1, 446.8, 444.0, 444.0, 447.45, 449.65, 447.9, 459.0, 481.1, 483.05, 481.65, 486.55, 496.35, 501.85, 510.65, 528.8, 499.35, 497.8, 485.05, 481.1, 489.9, 485.6, 476.6, 489.7, 489.2, 490.3, 477.25, 474.55, 475.75, 475.3, 477.25, 490.55, 489.45, 488.2, 478.4, 463.75, 472.25, 470.75, 469.35, 466.75, 464.55, 454.75, 443.8, 424.3, 422.25, 413.5, 416.65, 434.0, 435.1, 446.9, 447.35, 434.5, 437.65, 441.95, 444.05, 444.8, 442.6, 444.6, 446.0, 431.9, 420.9, 430.75, 435.3, 430.25, 428.15, 426.5, 427.8, 424.95, 440.55, 431.35, 433.8, 431.05, 422.95, 419.15, 418.8, 423.1, 423.0, 430.0, 434.1, 431.7, 423.9, 419.5, 400.1, 410.0, 409.75, 422.65, 404.2, 406.0, 412.0, 408.7, 409.0, 400.2, 404.0, 405.75, 410.95, 406.7, 413.25, 408.9, 408.95, 405.05, 402.9, 385.85, 383.55, 389.25, 393.85, 387.25, 381.55, 379.55, 381.75, 389.9, 377.55, 383.7, 390.0, 380.2, 381.2, 379.25, 380.05, 387.8, 384.8, 399.9, 400.15, 410.4, 397.2, 399.15, 395.25, 394.85, 396.3, 388.85, 385.15, 376.05, 376.0, 375.6, 368.15, 361.5, 357.9, 365.85, 360.3, 366.8, 373.35, 365.9, 368.05, 369.1, 373.05, 367.25, 378.4, 383.35, 380.15, 394.65, 391.9, 394.1, 392.45, 397.1, 386.85, 388.3, 380.95, 379.95, 388.85, 391.3, 389.0, 374.9, 374.15, 386.15, 386.35, 390.75, 395.5, 394.6, 396.15, 397.1, 398.8, 400.0, 399.45, 396.05, 401.4, 403.8, 399.25, 412.75, 399.5, 397.15, 392.2, 390.15, 386.55, 385.25, 386.3, 384.45, 372.4, 367.15, 360.9, 372.75, 373.1, 373.75, 372.0, 371.8, 370.5, 365.0, 369.25, 370.2, 361.1, 354.0, 351.95, 346.6, 347.8, 354.95, 354.2, 344.95, 344.95, 342.95, 341.85, 344.05, 344.4, 338.3, 332.05, 337.15, 345.05, 329.75, 329.9, 320.3, 311.35, 273.35, 280.5, 282.7, 280.95, 289.05, 296.1, 292.4, 293.4, 294.55, 280.85, 276.1, 295.25, 293.85, 292.2, 285.15, 291.4, 290.05, 293.0, 287.25, 291.15, 290.05, 287.3, 294.75, 292.1, 290.9, 302.3, 293.25, 298.9, 308.65, 313.05, 305.5, 311.6, 318.9, 315.35, 316.85, 324.85, 325.0, 325.75, 325.5, 319.05, 326.25, 329.05, 318.65, 317.1, 313.0, 308.45, 310.55, 307.15, 300.7, 294.8, 288.15, 283.55, 281.1, 282.35, 282.0, 294.65, 306.1, 308.9, 317.9, 349.55, 361.9, 355.8, 367.8, 368.35, 365.1, 358.8, 360.45, 357.85, 356.05, 357.8, 339.6, 336.8, 343.65, 338.7, 340.35, 340.75, 343.95, 343.35, 339.65, 336.35, 338.4, 358.45, 362.2, 361.95, 366.05, 368.15, 375.1, 369.75, 359.8, 372.25, 372.7, 370.6, 373.2, 369.3, 365.85, 362.9, 357.55, 335.85, 339.45, 342.95, 338.7, 340.5, 366.95, 356.7, 358.3, 361.0, 368.6, 368.15, 361.7, 360.1, 355.0, 342.55, 343.55, 348.6, 346.05, 332.1, 324.95, 334.15, 322.55, 324.15, 323.4, 325.85, 321.4, 317.0, 313.75, 305.05, 304.1, 305.15, 321.1, 324.4, 339.75, 341.0, 349.6, 349.55, 341.65, 339.45, 343.15, 340.65, 339.7, 353.3, 364.35, 363.4, 361.05, 356.1, 357.75, 347.8, 359.45, 356.4]\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 14s 252ms/step - loss: 0.0275 - val_loss: 0.0078\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 0.0040 - val_loss: 0.0067\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 6s 207ms/step - loss: 0.0032 - val_loss: 0.0065\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0032 - val_loss: 0.0065\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 6s 182ms/step - loss: 0.0030 - val_loss: 0.0062\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 6s 185ms/step - loss: 0.0029 - val_loss: 0.0060\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0028 - val_loss: 0.0057\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 7s 212ms/step - loss: 0.0027 - val_loss: 0.0061\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 0.0025 - val_loss: 0.0055\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0025 - val_loss: 0.0053\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0023 - val_loss: 0.0051\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 7s 223ms/step - loss: 0.0022 - val_loss: 0.0054\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 5s 171ms/step - loss: 0.0024 - val_loss: 0.0048\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 6s 202ms/step - loss: 0.0021 - val_loss: 0.0051\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 5s 172ms/step - loss: 0.0022 - val_loss: 0.0048\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0023 - val_loss: 0.0045\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 6s 204ms/step - loss: 0.0021 - val_loss: 0.0047\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0019 - val_loss: 0.0044\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0021 - val_loss: 0.0046\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0020 - val_loss: 0.0040\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 6s 207ms/step - loss: 0.0019 - val_loss: 0.0039\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0018 - val_loss: 0.0045\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 6s 201ms/step - loss: 0.0019 - val_loss: 0.0036\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 5s 172ms/step - loss: 0.0019 - val_loss: 0.0035\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0017 - val_loss: 0.0035\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 6s 207ms/step - loss: 0.0016 - val_loss: 0.0032\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0016 - val_loss: 0.0033\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0019 - val_loss: 0.0033\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0015 - val_loss: 0.0030\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0015 - val_loss: 0.0028\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0014 - val_loss: 0.0028\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 6s 200ms/step - loss: 0.0014 - val_loss: 0.0034\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 6s 175ms/step - loss: 0.0014 - val_loss: 0.0027\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0014 - val_loss: 0.0025\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 6s 207ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0013 - val_loss: 0.0024\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0011 - val_loss: 0.0020\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 5s 172ms/step - loss: 0.0011 - val_loss: 0.0020\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 6s 205ms/step - loss: 0.0011 - val_loss: 0.0020\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0011 - val_loss: 0.0019\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 6s 205ms/step - loss: 9.5486e-04 - val_loss: 0.0017\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 9.4531e-04 - val_loss: 0.0017\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 5s 172ms/step - loss: 9.9027e-04 - val_loss: 0.0017\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 9.1323e-04 - val_loss: 0.0015\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 8.3946e-04 - val_loss: 0.0014\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 6s 186ms/step - loss: 8.9685e-04 - val_loss: 0.0014\n",
            "31/31 [==============================] - 3s 43ms/step\n",
            "12/12 [==============================] - 1s 41ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d31c2acb9fe7>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  final_df = final_df.append({'Company': x, 'close price':ds[-1:], 'predicted price':output},ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[231.85, 229.3, 228.2, 228.25, 231.1, 232.95, 229.4, 229.4, 231.0, 232.25, 231.25, 227.7, 229.4, 223.5, 222.3, 223.8, 216.5, 214.05, 214.3, 211.3, 206.0, 209.85, 207.7, 204.3, 205.1, 202.4, 204.0, 200.5, 203.25, 201.55, 202.85, 201.8, 205.1, 201.8, 202.25, 201.65, 204.4, 200.1, 203.05, 202.75, 207.3, 206.85, 205.95, 205.15, 203.0, 200.4, 202.75, 200.1, 201.65, 203.05, 204.1, 201.45, 202.8, 201.8, 203.55, 200.65, 202.55, 203.2, 203.0, 207.25, 207.7, 209.0, 209.85, 211.45, 210.4, 211.7, 207.9, 209.3, 209.4, 207.45, 206.35, 205.5, 206.8, 206.65, 204.8, 204.8, 206.45, 206.15, 203.2, 207.55, 204.95, 204.1, 205.15, 204.05, 202.6, 205.25, 204.9, 206.25, 207.75, 205.85, 204.0, 205.15, 201.8, 204.55, 196.2, 198.2, 192.7, 195.2, 197.35, 201.1, 202.3, 197.7, 197.5, 202.3, 202.2, 199.85, 200.15, 198.6, 200.0, 199.9, 199.95, 202.15, 200.45, 199.75, 200.8, 201.2, 199.65, 196.75, 194.6, 196.0, 198.95, 197.55, 199.6, 199.95, 202.45, 199.95, 200.7, 200.5, 199.4, 198.2, 199.4, 199.55, 199.15, 198.85, 198.5, 202.65, 203.2, 202.8, 204.4, 202.0, 201.75, 201.35, 199.25, 199.5, 198.75, 198.1, 197.95, 199.75, 198.25, 197.2, 199.5, 196.35, 196.05, 194.1, 195.75, 196.4, 194.9, 194.8, 194.05, 193.25, 194.2, 193.55, 193.8, 198.55, 196.45, 195.4, 195.95, 197.55, 195.6, 196.05, 196.6, 199.65, 199.45, 202.65, 207.6, 205.9, 206.9, 206.45, 206.35, 205.3, 208.0, 208.35, 207.4, 209.3, 214.6, 219.5, 217.0, 216.95, 219.45, 217.95, 216.1, 216.6, 215.05, 214.65, 213.15, 210.3, 208.05, 202.55, 204.3, 207.4, 204.25, 205.4, 205.7, 204.0, 207.55, 205.25, 198.25, 194.05, 195.3, 193.2, 193.75, 193.2, 194.7, 196.3, 193.0, 193.9, 194.1, 192.45, 192.2, 189.5, 187.1, 187.85, 184.15, 183.25, 180.0, 178.15, 176.95, 177.0, 179.4, 184.0, 186.35, 182.9, 185.15, 187.05, 191.6, 190.95, 187.9, 192.4, 196.45, 199.5, 196.95, 197.95, 192.9, 191.85, 192.8, 195.75, 198.25, 197.0, 196.5, 197.9, 202.25, 200.65, 198.45, 200.15, 199.5, 202.0, 197.05, 191.3, 189.75, 191.5, 189.0, 186.45, 187.05, 188.6, 187.15, 188.6, 189.95, 192.65, 196.05, 194.8, 191.95, 189.05, 185.25, 180.25, 182.3, 176.95, 179.6, 174.15, 171.35, 170.4, 176.85, 178.1, 180.9, 181.6, 180.85, 179.05, 176.65, 175.85, 172.9, 170.95, 169.85, 169.85, 181.35, 178.05, 179.15, 180.55, 180.15, 177.75, 175.8, 174.65, 177.95, 174.9, 177.55, 175.55, 174.25, 172.95, 174.35, 175.1, 175.9, 182.9, 185.2, 188.45, 188.85, 193.1, 188.85, 189.5, 188.05, 191.8, 186.15, 183.75, 183.8, 187.8, 185.4, 185.3, 183.8, 186.95, 187.1, 186.55, 186.15, 185.45, 183.7, 182.3, 181.05, 178.35, 178.55, 173.95, 174.8, 172.8, 180.55, 176.75, 174.5, 175.05, 174.15, 171.15, 170.9, 170.0, 172.8, 167.8, 166.85, 167.2, 170.9, 168.15, 167.75, 168.25, 175.05, 175.25, 174.05, 173.95, 173.05, 175.3, 178.0, 176.2, 180.0, 180.8, 177.25, 177.0, 178.9, 174.65, 176.45, 175.15, 175.8, 182.15, 182.45, 183.6, 185.1, 186.9, 186.8, 184.45, 184.7, 185.35, 190.25, 190.85, 189.15, 187.0, 190.3, 189.35, 189.85, 186.1, 188.95, 185.25, 182.85, 183.2, 183.65, 180.35, 180.45, 176.35, 176.1, 176.85, 181.0, 181.25, 177.05, 171.45, 171.15, 169.15, 175.5, 177.8, 175.7, 175.45, 175.35, 174.3, 174.1, 174.65, 171.4, 170.1, 170.05, 166.45, 169.1, 169.3, 166.4, 163.7, 162.2, 167.9, 166.1, 163.35, 162.05, 163.55, 164.15, 164.55, 172.45, 168.6, 172.0, 173.3, 165.75, 167.0, 163.65, 165.65, 168.6, 168.15, 167.65, 164.35, 163.95, 163.9, 166.9, 170.6, 176.4, 177.4, 173.5, 172.9, 168.25, 168.35, 171.95, 171.3, 169.45, 171.1, 173.6, 174.3, 178.55, 179.25, 177.35, 175.95, 178.1, 179.35, 178.15, 174.05, 173.65, 173.0, 170.0, 168.95, 165.0, 162.3, 156.3, 155.45, 156.65, 158.8, 156.9, 159.65, 158.6, 162.35, 166.55, 172.85, 175.9, 170.15, 166.3, 166.1, 163.55, 166.85, 164.1, 166.4, 160.95, 160.3, 154.9, 160.75, 163.4, 158.3, 160.55, 156.9, 161.35, 160.65, 160.85, 158.15, 160.35, 160.15, 156.8, 156.95, 157.1, 156.95, 158.0, 159.7, 160.85, 166.2, 161.85, 167.2, 164.5, 171.45, 174.5, 175.55, 175.35, 174.75, 174.05, 171.75, 172.2, 170.85, 171.9, 176.8, 177.65, 175.75, 177.25, 180.0, 180.35, 181.0, 181.65, 182.1, 183.95, 187.9, 186.15, 182.8, 185.05, 183.7, 180.85, 180.05, 181.05, 176.45, 179.0, 178.6, 175.65, 174.55, 170.2, 173.65, 172.1, 173.25, 177.6, 179.6, 172.35, 171.95, 173.15, 175.8, 171.25, 171.8, 165.45, 164.0, 161.6, 162.5, 162.75, 167.5, 167.5, 167.8, 163.05, 162.4, 165.35, 165.45, 166.75, 165.5, 167.5, 167.9, 168.15, 165.25, 165.95, 167.3, 164.9, 161.15, 161.9, 162.6, 162.5, 159.85, 157.0, 154.95, 156.15, 155.9, 151.45, 149.4, 148.15, 147.5, 143.4, 141.05, 142.1, 144.95, 143.6, 141.35, 141.05, 142.1, 142.2, 144.5, 145.5, 145.25, 146.8, 145.1, 144.05, 146.95, 152.75, 155.05, 156.0, 155.1, 157.65, 154.3, 148.25, 150.2, 149.2, 149.65, 149.55, 151.3, 152.15, 152.5, 153.7, 155.1, 157.6, 159.45, 161.3, 161.05, 160.9, 161.6, 160.85, 160.05, 159.85, 160.15, 158.95, 159.75, 158.8, 158.3, 157.05, 157.1, 155.9, 153.75, 152.9, 150.05, 151.15, 153.25, 157.9, 157.0, 157.5, 163.5, 166.25, 165.65, 167.35, 168.25, 169.95, 171.95, 172.2, 171.8, 170.0, 168.05, 173.75, 171.4, 170.5, 168.6, 168.95, 171.85, 172.3, 171.45, 168.05, 168.8, 170.35, 168.55, 171.5, 172.25, 171.65, 172.0, 174.35, 170.65, 171.15, 168.3, 169.0, 166.35, 164.1, 168.75, 166.45, 164.95, 165.0, 164.35, 168.1, 170.15, 170.05, 170.85, 171.05, 171.55, 169.8, 166.5, 167.85, 168.9, 168.15, 167.7, 169.05, 168.7, 166.3, 169.15, 169.0, 169.3, 169.1, 167.75, 166.15, 165.35, 165.45, 168.15, 167.45, 168.65, 169.3, 170.0, 166.7, 167.5, 166.75, 166.9, 166.4, 168.35, 168.35, 162.0, 163.9, 159.4, 159.45, 158.55, 160.6, 162.4, 160.45, 158.6, 158.7, 161.1, 159.85, 156.9, 154.2, 153.2, 150.6, 152.8, 154.5, 155.45, 154.75, 154.05, 153.15, 154.3, 155.3, 155.5, 156.65, 156.45, 155.5, 157.5, 157.55, 158.9, 157.6, 157.45, 155.7, 153.8, 152.7, 154.05, 155.65, 156.2, 157.1, 164.5, 161.6, 164.35, 162.6, 160.35, 159.35, 157.65, 157.3, 155.8, 157.1, 156.95, 155.55, 155.45, 151.8, 148.05, 148.05, 147.65, 147.4, 148.25, 149.3, 149.95, 151.1, 153.5, 152.4, 152.0, 151.55, 149.85, 151.0, 153.85, 150.9, 150.05, 149.35, 149.55, 148.45, 148.15, 149.05, 149.6, 147.8, 146.8, 146.85, 145.95, 145.85, 144.3, 139.35, 137.95, 139.3, 142.45, 142.7, 141.95, 143.5, 144.1, 145.4, 144.8, 141.0, 140.85, 141.55, 142.8, 140.25, 140.6, 140.95, 143.95, 144.15, 145.85, 148.95, 149.05, 155.25, 154.9, 156.0, 157.35, 160.25, 159.3, 157.2, 157.85, 156.3, 156.3, 155.0, 154.15, 153.85, 153.95, 157.3, 156.45, 159.9, 159.55, 152.3, 154.75, 148.5, 150.0, 152.3, 155.1, 154.0, 156.6, 154.2, 152.2, 154.05, 153.95, 150.75, 147.8, 150.05, 150.2, 151.15, 149.7, 152.0, 150.65, 146.8, 145.0, 140.2, 143.0, 142.0, 138.05, 141.3, 140.3, 143.75, 148.15, 145.6, 144.15, 144.05, 143.65, 140.4, 142.3, 141.1, 143.85, 146.1, 142.35, 143.2, 147.55, 146.5, 143.7, 143.05, 143.3, 141.2, 140.25, 141.05, 142.0, 137.7, 130.7, 131.55, 129.85, 135.25, 134.85, 136.4, 137.1, 138.35, 143.75, 144.3, 139.65, 136.75, 136.15, 130.65, 131.15, 129.05, 132.4, 124.45, 126.3, 130.8, 130.9, 130.7, 128.05, 130.0, 130.45, 134.3, 138.6, 137.6, 139.45, 141.1, 142.95, 140.15, 144.9, 147.1, 148.3, 142.8, 145.45, 149.0, 149.7, 147.95, 147.45, 143.85, 143.9, 142.6, 142.25, 144.35, 145.15, 147.9, 144.05, 143.65, 145.1, 143.55, 145.85, 147.65, 148.75, 145.35, 143.3, 145.2, 144.05, 146.05, 147.25, 149.1, 146.25, 146.6, 147.25, 152.6, 152.7, 152.55, 150.3, 149.15, 149.5, 152.8, 156.05, 155.05, 154.65, 150.2, 150.85, 150.15, 148.75, 146.6, 147.9, 147.25, 147.05, 145.7, 145.35, 143.85, 148.95, 153.45, 136.05, 139.25, 139.65, 137.3, 137.95, 134.95, 135.45, 135.0, 135.95, 136.8, 137.8, 137.15, 137.25, 136.05, 137.0, 138.2, 134.9, 131.75, 132.15, 129.85, 129.55, 133.65, 131.4, 132.75, 131.25, 132.25, 133.15, 131.6, 131.85, 130.75, 132.65, 131.1, 131.5, 129.45, 128.5, 127.8, 128.2, 126.45, 126.0, 130.2, 132.4, 133.6, 135.95, 135.2, 132.15, 130.75, 130.1, 132.25, 133.85, 133.85, 132.35, 131.1, 132.45, 116.9, 115.85, 115.45, 112.4, 111.2, 113.85, 114.05, 116.7, 116.35, 117.0, 117.4, 117.15, 116.25, 117.05, 117.9, 115.95, 115.75, 113.85, 114.5, 113.95, 115.1, 114.9, 116.8, 120.95, 119.9, 121.95, 122.9, 123.0, 120.3, 119.4, 120.55, 123.7, 124.6, 125.4, 122.6, 122.95, 124.55, 123.8, 122.1, 118.5, 117.95, 117.65, 116.35, 114.45, 113.85, 114.3, 115.75, 115.55, 118.15, 121.1, 122.8, 125.5, 129.3, 131.6, 145.6, 142.25, 145.65, 154.2, 160.2, 156.25, 154.15, 151.45, 152.15, 160.5, 159.25, 164.2, 165.0, 164.2, 165.5, 165.35, 163.55, 158.1, 160.25, 153.35, 152.9, 154.9, 151.5, 152.5, 151.65, 152.55, 154.85, 156.85, 152.3, 152.25, 156.2, 154.6, 159.75, 157.35, 158.55, 161.35, 153.0, 150.65, 151.15, 149.7, 147.8, 145.6, 146.65, 150.4, 149.7, 148.0, 149.85, 149.6, 149.7, 148.95, 149.05, 149.8, 145.05, 140.75, 141.3, 140.7, 140.45, 140.85, 137.95, 135.85, 139.5, 139.95, 142.05, 142.6, 143.9, 145.3, 142.5, 142.5, 141.8, 141.0, 139.5, 137.4, 139.7, 140.2, 139.7, 142.45, 142.85, 140.45, 140.6, 139.4, 140.0, 137.75, 135.9, 132.15, 133.65, 136.9, 138.25, 136.45, 136.9, 138.0, 136.45, 139.35, 139.0, 139.1, 139.05, 140.65, 143.4, 142.9, 141.25, 140.25, 140.7, 138.25, 140.95, 143.5, 147.55, 146.55, 146.7, 145.85, 147.75, 146.25, 147.1, 150.1, 147.7, 145.25, 144.45, 146.15, 145.8, 142.2, 142.7, 143.1, 145.9, 146.8, 144.2, 142.5, 142.65, 143.05, 140.15, 141.8, 141.15, 142.7, 140.15, 140.45, 143.15, 142.05, 141.05, 139.9, 134.95, 135.15, 133.5, 132.95, 133.0, 128.3, 128.1, 132.95, 135.5, 139.0, 142.95, 139.35, 139.3, 140.1, 141.95, 144.0, 142.45, 144.4, 144.0, 139.2, 142.55, 144.95, 140.25, 140.0, 138.95, 137.75, 139.2, 140.4, 141.75, 142.85, 143.5, 140.2, 141.9, 143.15, 142.85, 141.85, 143.65, 142.05, 144.0, 142.8, 140.15, 140.0, 138.8, 137.15, 140.15, 142.1, 143.25, 144.0, 146.05, 146.65, 145.65, 146.05, 146.6, 146.05, 153.35, 158.45, 155.9, 159.5, 158.7, 157.5, 158.15, 151.8, 149.95, 153.75, 159.7, 159.45, 155.85, 157.85, 152.65, 154.75, 145.5, 150.15, 152.7, 147.35, 144.0, 145.6, 147.6, 146.85, 147.75, 152.1, 156.3, 156.1, 157.35, 155.65, 157.3, 154.35, 156.1, 158.0, 154.4, 151.35, 152.7, 149.5, 149.8, 148.25, 151.3, 151.6, 150.3, 152.35, 150.5, 144.6, 142.5, 142.0, 144.95, 141.65, 138.4, 139.05, 136.1, 136.15, 136.1, 136.15, 134.95, 135.35, 134.25, 132.5, 134.25, 134.65, 136.55, 138.55, 137.95, 138.45, 136.9, 140.05, 138.45, 139.0, 140.0, 136.55, 136.5, 135.85, 137.65, 137.0, 136.6, 136.05, 137.0, 139.25, 136.8, 135.6, 137.7, 138.65, 137.65, 138.3, 137.65, 138.85, 138.0, 134.45, 133.0, 131.1, 129.85, 133.1, 133.1, 133.15, 134.7, 135.55, 135.7, 135.6, 137.55, 138.35, 138.0, 134.2, 137.05, 135.5, 135.85, 135.05, 135.05, 135.15, 136.75, 135.9, 133.85, 131.35, 130.8, 128.05, 127.85, 129.65, 128.05, 127.25, 126.0, 125.95, 123.4, 115.3, 119.15, 119.9, 119.15, 121.6, 121.45, 120.25, 116.5, 117.6, 114.5, 112.6, 114.45, 120.9, 119.25, 119.65, 125.9, 126.85, 127.85, 126.8, 126.75, 120.9, 121.3, 122.05, 120.95, 121.35, 123.75, 123.5, 127.7, 125.75, 127.75, 126.0, 126.0, 127.25, 127.45, 125.85, 125.0, 126.8, 127.3, 127.95, 129.95, 132.65, 132.85, 132.65, 129.45, 127.95, 132.8, 132.7, 135.7, 135.05, 136.0, 135.55, 133.55, 131.35, 131.9, 130.8, 131.2, 132.1, 132.9, 133.95, 134.15, 133.1, 131.15, 132.9, 132.7, 130.9, 132.65, 133.1, 133.5, 130.4, 130.2, 130.05, 130.15, 133.2, 132.75, 132.85, 131.8, 133.55, 135.55, 135.85, 136.4, 137.0, 139.0, 139.3, 143.95, 144.8, 145.05, 146.25, 144.5, 143.3, 142.25, 141.45, 137.85, 139.55, 141.6, 145.2, 144.25, 143.2, 136.45, 134.6, 135.15, 132.2, 133.0, 138.15, 137.95, 143.75, 144.25, 142.45, 140.25, 134.05, 128.65, 125.9, 124.8, 123.25, 125.85, 124.75, 120.95, 123.95, 124.8, 127.75, 128.0, 127.5, 129.8, 127.0, 123.8, 118.65, 118.5, 121.55, 119.5, 122.85, 126.8, 126.4, 126.05, 127.55, 126.55, 126.8, 126.4, 127.4, 127.3, 127.55, 126.85, 127.45, 127.15, 128.25, 127.35, 129.15, 127.3, 129.9, 128.85]\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 13s 228ms/step - loss: 0.0310 - val_loss: 0.0049\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 0.0048 - val_loss: 0.0024\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 6s 180ms/step - loss: 0.0033 - val_loss: 0.0022\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0031 - val_loss: 0.0022\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0023 - val_loss: 0.0018\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0022 - val_loss: 0.0018\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 7s 218ms/step - loss: 0.0022 - val_loss: 0.0017\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 6s 196ms/step - loss: 0.0021 - val_loss: 0.0015\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 6s 175ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 5s 170ms/step - loss: 0.0019 - val_loss: 0.0015\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0017 - val_loss: 0.0015\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 6s 196ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 6s 178ms/step - loss: 0.0015 - val_loss: 0.0012\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0012 - val_loss: 0.0011\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 9.9785e-04 - val_loss: 9.8361e-04\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 6s 194ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 6s 177ms/step - loss: 9.5122e-04 - val_loss: 9.6700e-04\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 9.7065e-04 - val_loss: 9.9190e-04\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 9.4442e-04 - val_loss: 9.7498e-04\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 5s 170ms/step - loss: 0.0011 - val_loss: 9.4717e-04\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 8.9713e-04 - val_loss: 9.3737e-04\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 8.4298e-04 - val_loss: 9.3890e-04\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 8.3725e-04 - val_loss: 8.7482e-04\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 8.2961e-04 - val_loss: 8.6229e-04\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 6s 198ms/step - loss: 8.0055e-04 - val_loss: 9.0733e-04\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 6s 176ms/step - loss: 8.5043e-04 - val_loss: 9.1608e-04\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 5s 165ms/step - loss: 7.9373e-04 - val_loss: 8.5312e-04\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 7.5996e-04 - val_loss: 8.3520e-04\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 7.4892e-04 - val_loss: 7.9449e-04\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 7.9602e-04 - val_loss: 7.7942e-04\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 5s 164ms/step - loss: 7.8863e-04 - val_loss: 8.1554e-04\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 7s 226ms/step - loss: 7.3626e-04 - val_loss: 7.5880e-04\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 7.0098e-04 - val_loss: 7.4727e-04\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 6s 188ms/step - loss: 7.3543e-04 - val_loss: 7.0841e-04\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 6s 182ms/step - loss: 7.8769e-04 - val_loss: 7.1520e-04\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 6.7499e-04 - val_loss: 6.9834e-04\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 7.3740e-04 - val_loss: 6.8298e-04\n",
            "31/31 [==============================] - 4s 54ms/step\n",
            "12/12 [==============================] - 1s 53ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d31c2acb9fe7>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  final_df = final_df.append({'Company': x, 'close price':ds[-1:], 'predicted price':output},ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1187.45, 1208.25, 1223.5, 1224.8, 1215.7, 1216.5, 1190.95, 1195.3, 1231.75, 1209.8, 1212.55, 1196.8, 1161.15, 1139.95, 1114.5, 1121.05, 1108.9, 1102.35, 1096.4, 1101.55, 1105.1, 1133.8, 1139.6, 1087.65, 1092.25, 1118.15, 1106.25, 1087.0, 1100.25, 1088.3, 1103.0, 1099.7, 1103.5, 1109.8, 1107.5, 1101.5, 1101.25, 1109.85, 1117.6, 1109.05, 1100.85, 1095.05, 1094.5, 1100.5, 1096.2, 1088.55, 1083.75, 1084.7, 1069.8, 1071.1, 1073.5, 1054.2, 1056.85, 1059.15, 1069.25, 1056.65, 1079.9, 1093.05, 1097.3, 1098.7, 1084.45, 1096.95, 1084.85, 1082.9, 1061.6, 1063.8, 1052.9, 1040.35, 1039.65, 1031.4, 1003.8, 1002.5, 1009.7, 1015.1, 1017.9, 1017.75, 1044.9, 1040.1, 1046.85, 1054.8, 1047.6, 1043.8, 1031.05, 1042.85, 1044.7, 1050.9, 1029.95, 1027.95, 1045.9, 1044.55, 1027.0, 1051.35, 1025.1, 1118.65, 1094.75, 1082.0, 1080.3, 1070.0, 1123.2, 1132.05, 1167.7, 1145.65, 1182.35, 1211.05, 1189.7, 1179.2, 1164.3, 1190.3, 1188.7, 1180.0, 1197.1, 1166.4, 1164.5, 1186.05, 1193.0, 1204.25, 1197.1, 1194.3, 1189.35, 1263.4, 1304.85, 1306.0, 1320.8, 1302.3, 1306.3, 1285.4, 1293.45, 1265.75, 1285.7, 1293.9, 1278.2, 1277.15, 1258.5, 1236.95, 1258.7, 1259.1, 1259.15, 1272.1, 1277.8, 1259.95, 1255.25, 1262.9, 1259.55, 1246.55, 1240.7, 1277.85, 1271.65, 1263.05, 1242.35, 1230.85, 1232.7, 1235.25, 1237.4, 1266.45, 1277.55, 1270.45, 1259.05, 1284.1, 1266.7, 1259.05, 1283.15, 1272.6, 1276.6, 1289.85, 1318.2, 1344.15, 1337.6, 1338.95, 1321.4, 1339.0, 1350.7, 1353.65, 1355.5, 1355.95, 1382.35, 1396.3, 1406.95, 1399.75, 1397.25, 1404.2, 1403.85, 1402.65, 1424.7, 1437.1, 1457.9, 1458.05, 1421.35, 1404.1, 1405.0, 1394.35, 1390.6, 1385.25, 1384.3, 1362.45, 1361.2, 1359.9, 1380.1, 1355.05, 1340.35, 1364.35, 1350.6, 1350.45, 1360.1, 1343.85, 1346.85, 1324.2, 1298.5, 1303.85, 1304.05, 1319.35, 1330.4, 1330.45, 1368.4, 1379.45, 1350.65, 1337.45, 1348.05, 1317.7, 1304.7, 1314.45, 1293.4, 1282.25, 1263.6, 1296.75, 1272.65, 1247.45, 1226.8, 1240.95, 1243.05, 1245.3, 1286.25, 1313.95, 1319.6, 1320.5, 1337.95, 1345.3, 1321.6, 1322.0, 1328.1, 1331.9, 1323.75, 1329.95, 1304.45, 1301.0, 1305.2, 1300.25, 1294.75, 1299.55, 1289.9, 1303.35, 1294.6, 1288.2, 1292.8, 1287.05, 1266.15, 1227.3, 1205.85, 1174.35, 1184.8, 1186.65, 1201.5, 1179.05, 1170.1, 1170.45, 1158.85, 1134.3, 1105.55, 1138.25, 1130.45, 1113.8, 1132.9, 1177.8, 1176.75, 1181.6, 1195.4, 1191.7, 1194.55, 282.05, 274.3, 273.6, 277.05, 279.45, 282.55, 279.65, 277.85, 268.4, 276.05, 271.95, 271.55, 264.95, 262.1, 270.35, 274.0, 269.85, 268.85, 263.85, 272.25, 271.3, 276.05, 281.75, 280.7, 271.15, 271.8, 272.35, 270.15, 269.95, 271.5, 272.65, 272.95, 275.9, 279.2, 283.85, 282.4, 291.3, 293.05, 297.9, 296.5, 298.1, 291.6, 287.45, 282.9, 289.0, 286.05, 287.4, 289.55, 295.2, 304.2, 303.4, 303.85, 311.4, 317.75, 307.8, 303.4, 293.95, 309.65, 309.95, 305.25, 304.2, 304.3, 300.3, 301.95, 304.65, 296.95, 277.4, 274.0, 277.7, 273.95, 267.4, 265.65, 262.4, 274.45, 282.95, 277.55, 281.05, 281.6, 281.1, 280.0, 275.85, 276.35, 270.05, 265.2, 266.85, 268.95, 270.0, 268.35, 269.1, 265.8, 256.2, 256.05, 260.8, 264.65, 272.8, 284.2, 284.8, 279.35, 274.25, 276.35, 277.25, 276.5, 272.75, 274.95, 277.15, 281.0, 281.65, 286.95, 283.3, 278.9, 276.95, 276.65, 275.15, 277.0, 279.6, 279.9, 276.8, 276.3, 277.2, 269.25, 273.4, 276.45, 277.0, 273.2, 277.05, 283.1, 285.4, 279.5, 277.55, 279.2, 273.4, 277.15, 276.25, 276.0, 286.45, 278.5, 278.9, 282.3, 277.95, 274.95, 263.3, 263.9, 256.95, 260.95, 262.45, 261.15, 262.2, 261.1, 258.05, 260.8, 260.6, 274.7, 269.45, 261.5, 261.05, 257.8, 257.8, 254.75, 259.1, 262.55, 268.55, 266.1, 268.05, 264.05, 263.95, 264.45, 274.3, 268.25, 270.55, 264.8, 266.75, 269.45, 263.7, 268.15, 266.6, 265.4, 276.35, 276.9, 276.55, 284.8, 277.65, 277.75, 278.85, 277.95, 276.8, 277.8, 271.15, 265.65, 261.2, 258.8, 262.95, 257.3, 259.8, 252.1, 251.85, 246.55, 255.0, 251.9, 261.7, 258.95, 266.75, 266.35, 268.1, 269.3, 273.3, 266.7, 261.4, 257.4, 255.4, 252.05, 251.1, 250.3, 247.1, 252.9, 261.2, 264.55, 262.0, 262.5, 262.65, 261.2, 257.85, 256.6, 258.35, 263.2, 266.7, 261.75, 255.65, 256.4, 257.2, 261.2, 259.6, 257.8, 259.7, 256.25, 262.1, 268.25, 271.8, 275.95, 275.35, 274.2, 272.55, 278.3, 275.8, 276.05, 272.45, 276.15, 281.65, 283.3, 286.25, 283.2, 281.3, 281.75, 280.85, 276.1, 279.15, 281.3, 281.25, 291.85, 292.65, 290.35, 284.55, 281.05, 283.4, 293.2, 288.2, 281.4, 281.15, 283.7, 282.6, 278.95, 282.9, 277.65, 287.55, 293.75, 286.55, 273.05, 272.65, 272.3, 271.55, 270.55, 267.4, 261.85, 265.5, 257.75, 260.4, 268.35, 269.85, 273.25, 274.3, 268.8, 264.65, 264.1, 261.1, 260.8, 259.1, 269.05, 271.3, 272.25, 267.0, 267.85, 268.35, 264.65, 264.15, 262.6, 264.2, 270.05, 267.5, 265.2, 259.4, 260.6, 258.25, 256.05, 259.4, 254.05, 251.65, 249.2, 247.95, 246.05, 248.6, 246.55, 244.95, 243.45, 257.7, 257.05, 253.85, 256.4, 252.1, 253.2, 246.15, 254.05, 253.75, 260.3, 263.3, 258.5, 255.8, 261.45, 268.7, 267.85, 266.0, 263.2, 267.6, 272.55, 275.2, 279.5, 272.6, 277.3, 277.25, 277.9, 284.85, 286.3, 290.45, 284.6, 278.45, 278.65, 277.95, 279.5, 277.95, 282.5, 285.1, 287.05, 283.9, 285.4, 284.65, 281.85, 281.45, 284.35, 283.8, 276.45, 277.55, 276.8, 286.05, 280.2, 276.85, 280.65, 281.5, 282.95, 281.8, 279.75, 278.95, 280.55, 285.75, 284.3, 282.05, 287.05, 286.2, 281.7, 286.85, 287.45, 281.15, 275.45, 275.8, 275.9, 272.4, 271.65, 273.3, 274.6, 275.65, 276.35, 278.35, 277.55, 280.15, 282.9, 282.0, 280.85, 280.75, 285.1, 294.5, 287.15, 283.8, 281.8, 275.2, 280.85, 277.95, 281.0, 284.2, 287.2, 282.1, 280.85, 275.65, 279.95, 277.15, 279.85, 279.25, 279.95, 278.8, 279.5, 278.4, 276.05, 276.65, 274.6, 273.95, 273.0, 268.5, 264.85, 265.75, 266.7, 267.4, 266.8, 265.25, 257.15, 255.5, 256.65, 253.5, 257.9, 254.05, 251.95, 252.65, 251.1, 251.2, 250.0, 249.75, 253.8, 264.95, 262.15, 265.25, 267.7, 268.7, 267.8, 266.45, 263.7, 259.65, 259.65, 259.5, 257.8, 256.25, 264.65, 265.1, 261.15, 256.15, 257.6, 259.0, 265.8, 268.0, 267.65, 274.6, 279.8, 284.8, 288.0, 290.1, 292.2, 302.25, 292.35, 305.25, 307.55, 303.8, 314.45, 337.7, 336.55, 337.4, 333.2, 336.35, 341.1, 334.95, 333.3, 338.5, 339.7, 332.2, 325.75, 325.7, 321.75, 319.85, 313.6, 308.4, 320.05, 323.5, 322.3, 322.75, 317.85, 329.7, 329.05, 322.1, 323.9, 317.9, 306.35, 315.25, 313.45, 314.9, 312.05, 316.6, 318.95, 320.3, 325.75, 324.95, 324.95, 316.8, 319.9, 319.15, 317.65, 309.85, 300.8, 298.05, 295.65, 303.95, 303.3, 311.4, 308.85, 317.2, 310.55, 307.65, 314.3, 312.85, 303.8, 302.9, 307.25, 308.8, 319.0, 331.9, 328.85, 334.35, 328.05, 326.45, 326.5, 324.15, 328.85, 326.7, 328.9, 323.75, 318.8, 319.95, 320.15, 321.55, 326.45, 328.65, 320.85, 326.75, 335.05, 337.9, 340.75, 332.7, 326.75, 329.15, 328.15, 324.95, 326.95, 333.95, 334.95, 333.9, 327.9, 317.0, 318.2, 324.6, 323.9, 319.3, 321.05, 308.2, 309.2, 309.1, 311.25, 312.85, 307.75, 308.85, 299.45, 307.75, 298.8, 311.4, 308.45, 320.25, 331.1, 328.0, 321.9, 311.1, 309.1, 316.05, 303.85, 302.15, 296.15, 304.75, 296.55, 296.1, 299.75, 300.8, 314.65, 316.95, 307.35, 312.15, 308.4, 305.45, 303.6, 297.6, 280.7, 290.7, 278.85, 277.9, 276.4, 267.35, 272.75, 273.1, 278.1, 274.95, 288.85, 271.8, 261.95, 256.6, 250.7, 269.65, 276.7, 268.05, 259.3, 243.7, 248.7, 249.15, 251.35, 251.1, 257.35, 270.1, 288.3, 292.7, 289.3, 278.95, 282.7, 287.2, 279.7, 282.95, 299.2, 289.5, 275.65, 274.5, 279.45, 276.95, 274.4, 267.85, 262.3, 266.5, 264.7, 266.1, 268.8, 273.7, 273.7, 276.55, 275.5, 275.2, 281.1, 283.55, 285.25, 286.05, 282.25, 283.8, 283.95, 286.8, 290.6, 288.6, 293.7, 287.8, 289.55, 283.85, 281.85, 283.7, 277.35, 270.2, 265.85, 268.85, 270.1, 279.05, 277.35, 276.7, 270.45, 278.2, 288.35, 287.15, 289.9, 291.95, 299.1, 294.9, 292.4, 286.9, 290.9, 294.05, 305.05, 298.05, 291.8, 283.3, 280.0, 276.65, 274.7, 281.25, 273.7, 284.8, 288.8, 285.75, 292.3, 292.05, 289.55, 288.9, 288.1, 281.7, 275.8, 280.95, 276.25, 273.2, 280.3, 284.05, 292.9, 290.25, 291.8, 286.95, 287.5, 287.15, 287.0, 289.8, 285.1, 283.2, 277.0, 275.25, 277.0, 272.2, 275.55, 267.8, 268.0, 270.25, 267.9, 269.3, 272.25, 275.95, 283.1, 273.5, 276.4, 277.75, 276.8, 279.4, 276.2, 276.95, 282.2, 281.75, 283.45, 291.35, 290.6, 295.85, 300.5, 308.85, 316.1, 319.9, 323.1, 316.95, 324.15, 322.75, 324.95, 313.8, 315.8, 306.4, 307.9, 321.1, 319.9, 325.3, 331.0, 329.4, 318.7, 323.8, 330.0, 326.25, 325.65, 325.6, 322.45, 321.95, 320.0, 319.35, 316.55, 322.35, 323.1, 326.35, 326.0, 320.95, 321.35, 320.3, 325.1, 329.3, 335.4, 337.2, 335.15, 334.0, 347.95, 358.6, 372.6, 368.1, 378.55, 384.15, 416.75, 400.1, 398.45, 392.8, 407.95, 405.3, 395.05, 384.25, 374.15, 378.15, 399.55, 416.75, 411.0, 419.15, 465.65, 453.15, 440.55, 427.3, 428.6, 422.65, 426.9, 445.1, 442.7, 420.75, 417.5, 437.6, 442.95, 436.85, 411.35, 411.2, 424.85, 423.1, 426.6, 418.15, 421.8, 415.55, 396.7, 404.95, 404.0, 396.15, 402.75, 412.65, 416.1, 413.25, 410.5, 406.7, 408.9, 405.3, 404.9, 401.1, 394.85, 398.95, 395.35, 386.35, 392.75, 405.25, 396.9, 400.05, 394.2, 393.7, 405.55, 402.0, 410.1, 431.3, 435.2, 424.0, 425.9, 427.05, 428.75, 418.0, 428.15, 435.0, 439.55, 444.55, 436.9, 437.5, 443.55, 455.15, 449.4, 445.4, 429.5, 428.65, 420.85, 408.75, 407.55, 413.9, 404.0, 419.1, 407.3, 413.7, 400.35, 412.05, 407.5, 408.55, 402.85, 396.75, 406.15, 411.45, 404.2, 406.75, 402.15, 397.1, 397.1, 419.0, 408.0, 399.45, 403.45, 394.65, 391.7, 392.8, 395.9, 405.15, 404.55, 403.6, 409.65, 394.2, 394.85, 393.65, 385.8, 393.25, 396.45, 390.65, 384.15, 385.75, 383.3, 380.9, 386.45, 385.9, 382.3, 379.4, 364.45, 360.5, 371.45, 370.8, 365.3, 368.3, 352.1, 361.1, 349.45, 337.1, 343.05, 335.55, 343.0, 344.95, 349.45, 357.55, 352.05, 344.0, 344.45, 346.9, 341.65, 341.35, 344.05, 349.2, 353.15, 333.15, 338.05, 341.8, 351.0, 347.7, 339.95, 344.05, 346.5, 347.1, 347.5, 350.3, 346.05, 352.9, 348.8, 350.25, 353.9, 351.9, 351.35, 349.85, 359.45, 368.55, 356.65, 351.0, 356.35, 354.65, 344.95, 346.45, 339.55, 340.45, 332.6, 333.45, 330.7, 324.85, 315.05, 317.15, 319.35, 324.75, 324.95, 320.05, 320.5, 320.55, 319.95, 316.0, 314.4, 309.5, 312.2, 313.9, 312.0, 314.2, 309.2, 311.9, 311.35, 314.4, 313.4, 309.65, 301.6, 304.95, 315.2, 306.35, 306.45, 317.25, 319.0, 313.0, 310.15, 309.75, 311.85, 317.15, 327.65, 330.0, 321.8, 320.35, 312.3, 307.95, 316.85, 309.35, 307.55, 306.3, 304.0, 328.35, 342.15, 327.9, 320.05, 315.5, 319.0, 309.65, 311.1, 316.15, 318.9, 321.75, 317.6, 314.55, 318.3, 323.05, 329.55, 322.5, 332.0, 327.9, 329.85, 324.1, 320.3, 307.85, 301.35, 308.2, 306.4, 301.45, 305.35, 301.0, 300.4, 303.95, 304.45, 306.0, 312.85, 319.15, 318.4, 318.9, 317.9, 312.95, 313.2, 309.05, 309.6, 313.9, 310.0, 309.2, 308.25, 304.95, 302.95, 298.7, 293.9, 290.6, 289.7, 291.2, 291.6, 292.8, 291.85, 281.25, 284.85, 283.85, 282.85, 271.5, 269.85, 269.75, 270.9, 272.7, 273.65, 266.35, 270.0, 270.95, 282.85, 275.05, 270.05, 269.35, 269.75, 270.4, 264.0, 260.8, 260.6, 255.8, 255.95, 227.7, 230.65, 224.85, 230.3, 243.65, 239.3, 234.1, 225.2, 229.15, 225.55, 226.05, 229.9, 230.4, 226.9, 228.35, 229.65, 229.3, 230.5, 239.2, 238.95, 234.5, 236.55, 227.5, 225.1, 226.4, 229.35, 232.35, 240.5, 247.35, 257.75, 257.65, 263.65, 263.4, 253.9, 255.0, 260.15, 264.1, 257.85, 253.15, 253.45, 257.3, 256.45, 247.9, 251.8, 250.85, 246.95, 247.4, 250.7, 252.1, 250.65, 251.45, 249.3, 236.7, 237.0, 228.9, 230.15, 232.35, 230.7, 233.0, 234.05, 231.7, 232.8, 234.85, 235.4, 234.15, 235.6, 235.8, 229.7, 227.2, 223.9, 215.75, 217.1, 219.8, 215.35, 213.4, 217.8, 226.65, 224.8, 223.25, 230.45, 230.55, 235.55, 233.85, 240.2, 239.75, 239.9, 241.75, 242.5, 238.05, 241.85, 237.55, 226.6, 230.15, 225.5, 221.35, 223.2, 223.25, 219.45, 214.4, 218.0, 213.3, 207.3, 216.25, 219.65, 218.4, 219.9, 226.25, 225.05, 216.55, 213.05, 220.0, 219.95, 214.05, 218.15, 213.5, 203.15, 193.25, 200.35, 199.95, 200.35, 209.1, 211.15, 214.4, 214.6, 209.4, 216.25, 216.05, 194.1, 191.5, 194.95, 197.2, 200.7, 201.85, 205.25, 205.7, 205.1, 208.25, 207.4, 206.5, 212.25, 214.6, 216.65, 216.1, 214.6, 213.95, 212.25, 217.9, 214.1]\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 13s 210ms/step - loss: 0.0252 - val_loss: 5.7168e-04\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0049 - val_loss: 1.2841e-04\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0036 - val_loss: 1.6882e-04\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0034 - val_loss: 1.9944e-04\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0044 - val_loss: 3.0662e-04\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 5s 176ms/step - loss: 0.0029 - val_loss: 1.2693e-04\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 6s 195ms/step - loss: 0.0026 - val_loss: 1.3450e-04\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0025 - val_loss: 1.2758e-04\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 0.0024 - val_loss: 1.2024e-04\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0024 - val_loss: 1.3843e-04\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0022 - val_loss: 1.3244e-04\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 5s 171ms/step - loss: 0.0022 - val_loss: 1.3164e-04\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 7s 226ms/step - loss: 0.0028 - val_loss: 1.9278e-04\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0027 - val_loss: 1.6854e-04\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 6s 188ms/step - loss: 0.0021 - val_loss: 1.6178e-04\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 6s 184ms/step - loss: 0.0023 - val_loss: 1.3564e-04\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0019 - val_loss: 1.1469e-04\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0017 - val_loss: 1.2063e-04\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0015 - val_loss: 1.3118e-04\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 6s 212ms/step - loss: 0.0015 - val_loss: 1.3148e-04\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0017 - val_loss: 1.4117e-04\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0014 - val_loss: 1.6160e-04\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 5s 166ms/step - loss: 0.0015 - val_loss: 9.9675e-05\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 6s 199ms/step - loss: 0.0013 - val_loss: 9.9684e-05\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 6s 179ms/step - loss: 0.0013 - val_loss: 1.2616e-04\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0013 - val_loss: 8.9473e-05\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0011 - val_loss: 1.0894e-04\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0010 - val_loss: 9.2191e-05\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 9.6992e-04 - val_loss: 9.4407e-05\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 9.2249e-04 - val_loss: 8.7622e-05\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 0.0010 - val_loss: 1.0101e-04\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 8.4063e-04 - val_loss: 7.9511e-05\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 8.4596e-04 - val_loss: 8.3294e-05\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 8.1630e-04 - val_loss: 8.6548e-05\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 5s 173ms/step - loss: 8.1346e-04 - val_loss: 8.4222e-05\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 6s 201ms/step - loss: 0.0010 - val_loss: 1.0202e-04\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 8.2507e-04 - val_loss: 8.0452e-05\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 8.1957e-04 - val_loss: 9.6065e-05\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 9.5435e-04 - val_loss: 9.2714e-05\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 8.6258e-04 - val_loss: 7.9398e-05\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 5s 170ms/step - loss: 8.8191e-04 - val_loss: 8.8918e-05\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 7s 225ms/step - loss: 7.8196e-04 - val_loss: 8.0891e-05\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 8.3878e-04 - val_loss: 7.4978e-05\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 6s 203ms/step - loss: 8.5807e-04 - val_loss: 6.5511e-05\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 5s 173ms/step - loss: 7.0578e-04 - val_loss: 7.1246e-05\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 5s 174ms/step - loss: 6.6050e-04 - val_loss: 7.7020e-05\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 6s 204ms/step - loss: 7.6689e-04 - val_loss: 7.0284e-05\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 5s 170ms/step - loss: 6.5004e-04 - val_loss: 6.4403e-05\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 7s 212ms/step - loss: 7.8586e-04 - val_loss: 1.2545e-04\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 8.3235e-04 - val_loss: 6.2254e-05\n",
            "31/31 [==============================] - 2s 43ms/step\n",
            "12/12 [==============================] - 1s 43ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d31c2acb9fe7>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  final_df = final_df.append({'Company': x, 'close price':ds[-1:], 'predicted price':output},ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[112.4, 112.75, 113.55, 114.55, 114.85, 114.9, 113.25, 114.95, 117.35, 118.65, 117.6, 116.25, 116.9, 113.65, 114.3, 115.3, 113.3, 113.9, 112.7, 112.95, 113.7, 112.95, 112.0, 107.4, 107.75, 108.5, 108.1, 107.3, 107.05, 104.9, 106.75, 107.1, 108.15, 107.15, 107.0, 106.6, 106.25, 106.45, 107.55, 107.55, 108.7, 108.8, 108.25, 110.05, 108.8, 109.1, 106.8, 106.3, 104.65, 104.95, 106.35, 106.1, 108.2, 107.55, 107.95, 106.45, 106.75, 106.75, 106.7, 107.15, 106.65, 107.55, 107.2, 110.55, 111.15, 109.65, 108.45, 107.75, 107.65, 107.9, 107.55, 107.95, 110.05, 108.65, 109.65, 110.4, 113.2, 114.9, 110.45, 110.05, 109.35, 107.5, 107.25, 106.75, 106.0, 108.1, 106.75, 106.85, 108.35, 106.25, 103.65, 104.0, 102.7, 104.1, 103.35, 102.75, 100.9, 103.2, 101.35, 103.05, 103.3, 104.05, 104.4, 105.6, 105.75, 104.45, 103.05, 102.45, 101.95, 101.4, 103.05, 103.3, 102.65, 102.85, 102.85, 103.4, 102.6, 102.3, 101.45, 101.35, 103.25, 102.55, 104.1, 103.25, 103.55, 102.1, 102.05, 102.7, 102.0, 101.95, 100.95, 101.0, 100.7, 100.75, 100.35, 100.05, 99.85, 99.9, 101.45, 101.0, 101.0, 101.15, 101.25, 100.35, 100.4, 100.55, 99.9, 100.15, 98.8, 99.55, 102.15, 101.5, 100.15, 99.75, 101.9, 105.3, 105.0, 103.95, 104.35, 103.2, 104.35, 103.05, 103.05, 105.45, 104.35, 108.75, 108.8, 108.2, 107.6, 108.0, 108.05, 106.7, 106.5, 104.5, 105.55, 105.05, 106.85, 105.65, 105.25, 105.65, 105.65, 105.85, 105.05, 105.55, 105.85, 106.9, 108.65, 106.5, 108.5, 109.7, 111.9, 111.75, 109.25, 109.5, 109.9, 109.45, 109.85, 106.95, 106.75, 106.75, 105.35, 104.25, 105.6, 106.9, 103.75, 102.2, 101.85, 99.95, 100.4, 100.65, 100.1, 100.45, 101.5, 102.05, 98.35, 103.8, 102.0, 101.25, 99.55, 101.2, 100.15, 101.4, 101.9, 101.45, 98.45, 97.1, 96.55, 96.45, 93.85, 94.95, 96.85, 99.0, 99.8, 99.7, 98.95, 97.85, 95.6, 96.1, 97.9, 97.05, 96.65, 96.1, 95.85, 97.15, 97.95, 97.25, 97.75, 96.7, 96.15, 95.7, 96.6, 98.3, 98.3, 98.4, 98.1, 99.0, 97.85, 96.15, 96.05, 97.25, 96.75, 97.6, 97.75, 98.65, 97.9, 98.9, 98.85, 98.55, 98.4, 98.8, 96.55, 96.55, 95.65, 95.55, 98.65, 97.3, 97.8, 95.7, 95.9, 95.4, 96.4, 98.55, 98.75, 98.6, 98.6, 98.1, 98.95, 98.45, 99.6, 98.25, 99.2, 98.95, 99.5, 99.3, 99.0, 98.05, 97.95, 98.05, 98.4, 98.05, 98.8, 97.75, 98.6, 98.35, 97.95, 97.4, 97.35, 97.45, 97.95, 98.15, 99.1, 99.8, 101.0, 101.9, 104.85, 105.9, 105.2, 105.05, 104.95, 104.9, 104.25, 105.25, 105.05, 104.2, 103.45, 104.25, 104.6, 104.55, 104.3, 104.35, 104.05, 104.55, 106.4, 104.0, 104.3, 101.85, 103.1, 102.7, 102.35, 104.15, 101.4, 101.35, 101.8, 101.5, 101.35, 98.85, 101.0, 99.6, 98.8, 97.05, 95.55, 97.65, 99.0, 101.05, 100.65, 99.25, 101.0, 101.5, 101.45, 101.35, 101.5, 101.2, 102.55, 103.7, 101.8, 102.0, 102.7, 101.95, 101.35, 102.3, 102.25, 103.65, 108.35, 109.1, 110.1, 109.5, 110.25, 110.45, 109.6, 109.05, 109.95, 109.2, 108.0, 108.2, 110.2, 109.1, 110.25, 109.35, 109.85, 109.15, 108.0, 109.55, 108.5, 109.0, 107.15, 105.55, 105.1, 107.45, 107.95, 104.65, 104.5, 102.3, 102.45, 101.9, 103.55, 103.0, 103.6, 103.6, 103.15, 102.65, 101.05, 103.2, 101.2, 102.05, 99.7, 97.45, 99.95, 100.2, 99.35, 97.15, 95.35, 96.8, 96.2, 96.1, 94.35, 94.6, 94.95, 96.0, 99.65, 97.95, 98.8, 99.25, 96.25, 96.05, 95.25, 96.65, 97.35, 97.45, 98.45, 97.35, 95.85, 95.9, 97.25, 98.05, 98.7, 99.1, 99.35, 98.9, 97.85, 98.15, 99.95, 99.0, 99.0, 100.75, 103.9, 103.95, 105.0, 104.85, 103.65, 103.1, 104.1, 103.85, 103.85, 103.55, 105.45, 105.35, 103.6, 104.2, 103.15, 100.95, 99.05, 97.9, 95.05, 95.3, 96.4, 97.1, 95.2, 99.1, 100.0, 103.4, 102.65, 103.15, 99.4, 98.55, 96.6, 99.1, 96.35, 99.35, 98.1, 99.15, 97.55, 97.9, 100.65, 101.25, 100.85, 98.95, 100.2, 100.0, 100.1, 100.0, 100.9, 101.5, 101.3, 99.75, 99.85, 101.4, 102.6, 100.2, 100.55, 101.95, 100.15, 100.5, 99.1, 99.95, 101.45, 101.0, 100.55, 100.85, 103.15, 101.75, 104.1, 102.95, 105.1, 106.95, 109.0, 107.75, 107.7, 108.8, 107.35, 109.65, 109.4, 110.7, 110.7, 111.7, 110.55, 109.95, 112.65, 114.3, 111.55, 111.8, 113.0, 112.5, 112.25, 111.9, 110.3, 111.05, 108.6, 110.15, 110.85, 111.35, 112.6, 110.35, 107.8, 107.85, 107.45, 108.75, 106.25, 107.7, 105.95, 105.75, 106.2, 105.8, 108.15, 108.6, 110.1, 110.85, 109.7, 110.4, 111.7, 111.75, 112.0, 111.85, 113.6, 113.85, 114.65, 113.85, 110.75, 110.0, 108.45, 108.1, 108.25, 108.6, 111.25, 109.6, 108.65, 106.75, 105.8, 105.75, 104.25, 102.5, 101.35, 101.25, 102.9, 104.55, 104.75, 105.1, 103.95, 103.7, 103.6, 105.35, 104.1, 106.4, 107.35, 106.1, 106.15, 103.65, 104.05, 105.95, 109.65, 108.05, 107.0, 107.25, 107.65, 107.6, 105.7, 106.05, 107.05, 108.75, 108.4, 109.75, 110.05, 107.85, 109.85, 109.8, 109.7, 113.55, 112.6, 111.65, 113.0, 114.3, 113.0, 112.8, 113.15, 113.65, 112.45, 112.05, 112.65, 112.25, 111.2, 112.0, 111.25, 109.3, 110.95, 110.75, 111.75, 114.35, 118.15, 118.95, 119.0, 118.1, 118.3, 120.45, 118.55, 118.95, 119.05, 119.55, 119.9, 120.05, 120.1, 117.65, 119.55, 118.95, 117.45, 117.1, 118.3, 121.5, 119.05, 121.75, 119.45, 121.55, 123.95, 122.7, 121.1, 121.9, 121.7, 121.95, 120.95, 121.4, 121.15, 120.0, 117.6, 117.65, 116.75, 120.25, 118.1, 117.4, 117.0, 116.75, 120.5, 120.05, 117.65, 118.0, 118.2, 118.7, 120.2, 119.45, 119.15, 119.95, 118.85, 118.6, 119.75, 120.6, 119.4, 118.6, 117.45, 116.7, 113.85, 114.95, 113.2, 114.0, 116.95, 118.0, 117.05, 119.65, 120.0, 118.85, 119.55, 120.0, 119.4, 119.9, 119.1, 120.05, 122.1, 120.1, 119.1, 119.75, 120.4, 118.55, 117.9, 117.9, 118.95, 120.45, 120.15, 120.05, 118.7, 118.55, 117.5, 117.25, 118.25, 116.25, 116.35, 117.1, 116.2, 115.8, 114.7, 114.4, 115.3, 113.45, 113.5, 114.75, 113.85, 113.1, 114.15, 114.95, 114.4, 116.45, 115.55, 113.9, 113.15, 112.05, 111.05, 112.05, 112.65, 114.95, 113.9, 113.5, 112.15, 110.95, 111.0, 111.9, 112.1, 111.5, 110.2, 110.45, 111.2, 109.6, 109.35, 111.6, 111.25, 112.55, 112.85, 109.9, 105.7, 105.65, 108.25, 108.4, 108.15, 106.85, 109.05, 112.05, 110.95, 108.9, 104.3, 107.95, 108.45, 108.6, 107.65, 108.2, 110.0, 110.4, 109.25, 109.8, 109.65, 109.0, 108.75, 107.1, 104.35, 101.6, 102.45, 103.9, 105.0, 105.8, 105.5, 106.5, 106.2, 104.5, 104.65, 104.15, 103.3, 104.6, 103.55, 105.5, 104.05, 106.9, 106.9, 107.0, 109.4, 109.7, 109.85, 110.3, 110.45, 112.2, 114.9, 112.0, 112.1, 112.7, 114.75, 113.5, 112.35, 112.2, 112.2, 114.35, 113.75, 113.9, 116.05, 114.3, 112.7, 112.65, 109.6, 109.0, 112.15, 112.5, 113.5, 114.4, 113.3, 111.4, 113.2, 112.45, 112.05, 110.65, 110.0, 107.85, 108.0, 105.75, 109.1, 108.2, 108.4, 107.5, 105.85, 107.2, 105.9, 103.1, 106.0, 106.55, 111.3, 112.9, 111.35, 108.3, 107.65, 107.6, 105.3, 109.85, 108.15, 110.35, 110.3, 110.05, 110.2, 110.9, 111.0, 111.35, 111.15, 110.15, 110.0, 110.15, 108.35, 108.1, 105.55, 101.1, 102.85, 91.35, 93.3, 93.55, 93.3, 96.0, 95.1, 96.15, 97.1, 98.0, 96.5, 96.45, 97.6, 98.1, 97.8, 97.95, 95.7, 93.85, 97.3, 97.65, 99.0, 95.65, 96.35, 96.2, 98.95, 102.25, 99.0, 99.0, 99.75, 102.55, 101.55, 100.4, 100.4, 101.6, 99.25, 99.15, 100.5, 99.3, 98.95, 98.05, 99.0, 99.9, 97.95, 98.05, 98.25, 99.05, 99.25, 98.95, 99.55, 98.95, 97.6, 99.5, 99.5, 101.95, 100.6, 99.0, 100.2, 98.75, 100.4, 100.15, 101.3, 97.9, 98.4, 95.15, 96.25, 95.05, 96.05, 95.95, 95.35, 96.0, 96.0, 96.35, 95.05, 93.75, 93.4, 93.9, 94.15, 94.25, 92.15, 94.7, 95.1, 93.6, 93.75, 95.6, 96.3, 98.95, 101.3, 98.4, 98.0, 97.95, 97.5, 99.0, 98.7, 100.0, 99.25, 99.4, 99.7, 98.55, 99.0, 99.8, 99.0, 99.9, 99.5, 101.9, 100.7, 99.9, 98.8, 98.75, 98.85, 97.45, 98.0, 97.45, 97.5, 98.25, 97.25, 98.5, 97.85, 98.1, 98.05, 98.1, 96.55, 96.6, 95.7, 95.6, 95.5, 95.2, 95.35, 96.75, 96.2, 97.5, 97.0, 95.6, 95.7, 95.0, 95.25, 95.55, 95.75, 94.9, 95.1, 95.3, 95.05, 95.3, 95.2, 94.5, 94.7, 96.15, 96.35, 97.6, 99.0, 98.9, 98.85, 98.5, 98.8, 98.35, 101.45, 102.3, 99.3, 100.85, 101.25, 103.2, 105.6, 104.0, 105.5, 107.45, 105.0, 108.0, 108.4, 108.7, 106.6, 106.9, 106.75, 107.8, 107.65, 107.85, 107.7, 107.2, 105.95, 105.95, 105.6, 105.5, 106.1, 105.6, 105.8, 104.55, 105.1, 102.5, 102.5, 103.55, 106.85, 111.5, 112.85, 112.0, 114.45, 119.5, 122.25, 124.9, 124.45, 125.4, 130.85, 128.15, 126.9, 124.85, 125.05, 122.0, 125.2, 125.25, 124.25, 126.9, 128.1, 137.35, 135.95, 133.75, 136.45, 133.4, 131.8, 133.5, 133.95, 131.2, 132.3, 131.4, 132.4, 132.4, 132.55, 136.55, 139.2, 140.0, 141.3, 140.5, 144.2, 146.05, 137.75, 136.2, 137.55, 132.7, 133.95, 135.15, 136.25, 138.7, 137.1, 136.9, 135.75, 136.3, 134.65, 134.8, 133.2, 134.7, 133.1, 129.8, 132.45, 132.45, 134.05, 134.5, 131.9, 131.95, 132.25, 132.3, 133.2, 134.8, 136.55, 137.1, 136.45, 134.15, 132.45, 131.0, 129.5, 129.2, 132.4, 132.5, 132.75, 134.15, 134.45, 133.1, 134.05, 136.15, 137.85, 135.15, 136.75, 135.95, 136.15, 136.55, 136.85, 137.0, 136.5, 135.85, 136.0, 137.0, 139.55, 135.25, 135.95, 137.25, 137.9, 138.05, 136.05, 134.85, 136.05, 133.95, 134.1, 135.1, 139.95, 139.8, 139.85, 142.05, 142.5, 142.55, 144.45, 145.8, 144.85, 142.45, 144.9, 148.1, 147.65, 142.95, 144.65, 144.1, 146.75, 148.35, 146.65, 146.15, 146.55, 142.45, 139.1, 139.15, 140.75, 142.7, 138.35, 138.05, 137.2, 136.5, 136.75, 135.85, 134.95, 137.25, 135.95, 134.8, 134.45, 133.05, 131.0, 131.6, 135.8, 136.85, 139.05, 136.3, 137.1, 137.0, 137.65, 138.0, 137.1, 138.75, 138.9, 135.8, 137.75, 138.35, 138.45, 136.5, 135.85, 136.05, 142.95, 147.35, 147.85, 147.4, 146.9, 148.35, 149.55, 149.95, 149.45, 148.8, 148.0, 147.85, 147.2, 146.25, 143.65, 143.45, 141.45, 145.1, 145.75, 150.25, 151.5, 152.75, 154.75, 154.3, 153.25, 155.9, 156.75, 154.55, 152.75, 157.1, 156.85, 156.7, 155.55, 154.45, 155.15, 150.25, 148.5, 147.9, 149.65, 146.6, 146.75, 147.65, 146.65, 146.1, 148.2, 149.2, 147.95, 146.5, 145.1, 144.75, 145.0, 145.25, 146.45, 147.95, 147.95, 147.3, 151.0, 152.7, 154.35, 151.55, 150.7, 150.1, 149.8, 148.85, 147.05, 146.65, 145.85, 144.95, 144.45, 143.05, 142.2, 144.15, 143.15, 140.6, 140.3, 140.0, 143.0, 139.3, 140.85, 139.4, 140.05, 140.05, 138.75, 140.8, 139.5, 140.95, 143.1, 143.05, 143.2, 143.2, 143.7, 143.1, 142.5, 143.2, 144.35, 144.9, 146.15, 147.35, 147.15, 146.0, 145.4, 141.2, 144.1, 141.8, 141.95, 140.0, 141.3, 139.4, 139.85, 141.3, 139.1, 139.5, 139.1, 142.35, 141.5, 141.5, 140.0, 138.6, 138.0, 137.0, 138.65, 140.0, 140.2, 139.75, 139.6, 141.2, 140.2, 140.0, 142.8, 142.25, 142.05, 140.4, 141.35, 140.0, 142.4, 142.0, 140.6, 141.0, 140.75, 141.05, 139.6, 138.05, 137.95, 136.0, 137.8, 138.35, 138.1, 137.4, 136.45, 136.1, 133.4, 131.1, 128.6, 130.95, 134.85, 135.15, 130.65, 129.1, 125.2, 127.2, 123.6, 122.3, 126.05, 126.0, 123.35, 124.3, 127.55, 128.3, 131.1, 133.5, 136.55, 131.0, 127.8, 125.4, 126.8, 128.35, 132.35, 130.55, 132.5, 132.25, 132.55, 132.45, 132.55, 133.3, 133.05, 133.0, 135.05, 135.0, 133.8, 135.9, 135.9, 136.35, 135.75, 133.9, 131.05, 128.15, 128.75, 128.95, 131.65, 131.2, 130.95, 131.3, 130.05, 128.3, 129.45, 130.2, 129.0, 130.75, 130.9, 131.85, 130.9, 131.4, 131.85, 133.8, 133.5, 136.1, 136.0, 135.4, 133.8, 130.75, 131.15, 130.2, 130.15, 130.75, 128.0, 128.15, 129.5, 133.35, 134.55, 135.4, 139.45, 138.5, 140.05, 140.1, 142.15, 140.95, 141.0, 141.15, 141.0, 140.95, 139.15, 139.5, 138.1, 141.65, 143.25, 142.75, 142.9, 142.85, 139.1, 138.15, 135.95, 134.65, 132.2, 133.1, 133.0, 138.25, 143.7, 147.85, 146.75, 146.25, 143.25, 146.8, 145.25, 142.45, 141.95, 143.35, 136.7, 138.35, 138.7, 139.3, 139.6, 139.85, 141.0, 139.65, 137.1, 139.7, 131.75, 133.65, 130.05, 133.75, 135.4, 135.65, 138.65, 137.7, 139.75, 139.65, 138.2, 138.6, 138.95, 138.75, 138.0, 136.85, 138.0, 138.2, 137.45, 139.1, 138.15, 139.75, 139.1]\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 15s 254ms/step - loss: 0.0096 - val_loss: 0.0386\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 6s 206ms/step - loss: 0.0027 - val_loss: 0.0048\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 6s 200ms/step - loss: 0.0022 - val_loss: 0.0047\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 7s 211ms/step - loss: 0.0022 - val_loss: 0.0053\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 7s 222ms/step - loss: 0.0018 - val_loss: 0.0042\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 6s 196ms/step - loss: 0.0018 - val_loss: 0.0138\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 7s 225ms/step - loss: 0.0016 - val_loss: 0.0037\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 0.0014 - val_loss: 0.0037\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 7s 235ms/step - loss: 0.0014 - val_loss: 0.0051\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 0.0013 - val_loss: 0.0055\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 7s 228ms/step - loss: 0.0012 - val_loss: 0.0045\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 6s 192ms/step - loss: 0.0011 - val_loss: 0.0038\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 7s 226ms/step - loss: 0.0011 - val_loss: 0.0052\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 0.0011 - val_loss: 0.0035\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 7s 240ms/step - loss: 0.0011 - val_loss: 0.0046\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 0.0010 - val_loss: 0.0020\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 7s 227ms/step - loss: 9.4734e-04 - val_loss: 0.0020\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 9.1951e-04 - val_loss: 0.0064\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 7s 227ms/step - loss: 9.1473e-04 - val_loss: 0.0017\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 8.9145e-04 - val_loss: 0.0026\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 7s 232ms/step - loss: 8.8302e-04 - val_loss: 0.0053\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 7.8031e-04 - val_loss: 0.0031\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 7s 228ms/step - loss: 7.8828e-04 - val_loss: 0.0022\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 7.5989e-04 - val_loss: 0.0029\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 7s 228ms/step - loss: 7.5969e-04 - val_loss: 0.0025\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 7.1403e-04 - val_loss: 0.0029\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 7s 229ms/step - loss: 7.3555e-04 - val_loss: 0.0024\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 6s 190ms/step - loss: 6.9129e-04 - val_loss: 0.0011\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 7s 225ms/step - loss: 6.3236e-04 - val_loss: 0.0012\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 5.9131e-04 - val_loss: 9.7058e-04\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 7s 226ms/step - loss: 5.7408e-04 - val_loss: 0.0011\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 6s 190ms/step - loss: 6.2522e-04 - val_loss: 0.0010\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 7s 243ms/step - loss: 5.7374e-04 - val_loss: 0.0012\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 5.7439e-04 - val_loss: 0.0019\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 7s 227ms/step - loss: 6.7879e-04 - val_loss: 0.0013\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 6s 188ms/step - loss: 5.6900e-04 - val_loss: 9.6635e-04\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 7s 227ms/step - loss: 6.1034e-04 - val_loss: 0.0024\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 6s 194ms/step - loss: 5.8203e-04 - val_loss: 0.0013\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 7s 230ms/step - loss: 6.0495e-04 - val_loss: 0.0024\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 6s 188ms/step - loss: 5.7924e-04 - val_loss: 0.0014\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 7s 228ms/step - loss: 5.3482e-04 - val_loss: 0.0011\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 5.2303e-04 - val_loss: 0.0014\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 7s 228ms/step - loss: 5.7760e-04 - val_loss: 0.0013\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 5.5332e-04 - val_loss: 0.0012\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 7s 225ms/step - loss: 5.3566e-04 - val_loss: 0.0016\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 5.1934e-04 - val_loss: 0.0027\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 7s 240ms/step - loss: 5.2165e-04 - val_loss: 0.0013\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 5.5783e-04 - val_loss: 0.0019\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 7s 228ms/step - loss: 5.0618e-04 - val_loss: 0.0016\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 6s 189ms/step - loss: 5.4658e-04 - val_loss: 0.0021\n",
            "31/31 [==============================] - 3s 47ms/step\n",
            "12/12 [==============================] - 1s 45ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d31c2acb9fe7>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  final_df = final_df.append({'Company': x, 'close price':ds[-1:], 'predicted price':output},ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1075.5, 1070.7, 1088.0, 1106.05, 1103.15, 1081.5, 1084.35, 1088.6, 1121.7, 1110.75, 1099.5, 1085.0, 1077.5, 1054.05, 1053.85, 1042.05, 1027.55, 1037.15, 1046.2, 1045.4, 1027.3, 1033.95, 1019.15, 981.7, 993.75, 994.35, 993.25, 985.1, 1015.15, 1010.0, 1017.75, 1032.55, 997.8, 983.2, 977.95, 975.35, 978.8, 963.1, 978.95, 983.2, 1024.8, 1014.0, 1009.8, 1004.1, 990.35, 1008.75, 1016.45, 1021.6, 1028.05, 1067.95, 1069.7, 1076.95, 1092.05, 1073.95, 1091.4, 1091.45, 1100.45, 1093.3, 1088.15, 1074.25, 1092.05, 1125.9, 1123.2, 1129.7, 1106.55, 1124.7, 1127.35, 1120.9, 1090.7, 1084.25, 1061.8, 1063.2, 1054.1, 1075.75, 1088.2, 1071.25, 1061.25, 1017.25, 1035.05, 1033.6, 1023.75, 1021.15, 1020.65, 1007.85, 1032.8, 1079.4, 1067.8, 1082.95, 1071.8, 1043.65, 1016.55, 1020.6, 998.65, 999.95, 995.55, 1022.65, 985.65, 1008.05, 1022.25, 1036.65, 1045.6, 1010.55, 1011.05, 1030.8, 1031.2, 1007.75, 996.4, 1007.05, 1015.35, 1046.4, 1063.5, 1066.1, 1057.95, 1071.4, 1054.35, 1065.65, 1063.8, 1058.7, 1052.05, 1062.95, 1090.15, 1067.45, 1089.85, 1076.25, 1068.95, 1068.3, 1073.9, 1053.55, 1057.1, 1057.65, 1056.25, 1074.3, 1070.45, 1073.45, 1062.95, 1056.05, 1052.95, 1058.4, 1059.0, 1060.45, 1053.1, 1053.5, 1021.25, 1010.15, 1009.65, 1014.55, 1027.75, 1016.05, 1007.05, 1000.1, 993.6, 987.9, 983.25, 972.6, 979.1, 976.9, 970.65, 964.6, 977.2, 988.65, 976.9, 972.4, 967.8, 956.45, 949.6, 947.95, 919.2, 936.2, 937.45, 925.7, 952.65, 962.3, 959.6, 958.55, 992.5, 988.15, 1013.9, 1000.35, 1027.35, 1041.4, 1035.15, 1020.05, 997.65, 1001.65, 1010.2, 998.5, 999.0, 987.25, 1007.9, 1017.85, 1023.85, 1044.95, 1039.15, 1049.65, 1069.2, 1055.6, 1073.05, 1059.0, 1040.35, 1047.45, 1044.4, 1049.45, 1081.65, 1081.8, 1090.35, 1098.85, 1092.75, 1082.0, 1096.25, 1093.2, 1074.85, 1065.3, 1105.05, 1105.7, 1111.85, 1108.55, 1102.2, 1079.9, 1061.85, 1054.7, 1030.5, 1032.95, 996.25, 1012.25, 995.6, 996.05, 979.5, 962.35, 998.4, 985.6, 988.8, 1011.15, 1005.9, 1020.15, 1030.75, 1020.55, 984.5, 1023.9, 1041.75, 1055.25, 1053.9, 1056.85, 1056.75, 1073.15, 1056.05, 1059.4, 1059.7, 1055.25, 1047.9, 1048.6, 1051.2, 1058.7, 1055.6, 1077.1, 1075.8, 1085.6, 1065.4, 1033.45, 1013.75, 1030.8, 1015.35, 1001.5, 997.9, 994.85, 980.15, 969.85, 986.8, 971.6, 958.5, 942.7, 913.0, 919.3, 895.5, 921.3, 943.9, 920.0, 929.05, 915.25, 914.3, 899.65, 909.95, 914.8, 941.7, 944.15, 953.95, 935.55, 956.35, 984.85, 995.65, 964.2, 966.25, 964.25, 988.85, 977.4, 982.2, 977.05, 984.95, 993.85, 984.75, 992.05, 1018.35, 1037.25, 1047.1, 1030.85, 993.0, 990.3, 999.65, 1013.25, 1010.3, 1026.6, 1025.7, 1022.75, 1032.5, 1049.1, 1036.4, 1050.65, 1047.65, 1044.85, 1041.9, 1023.9, 1005.3, 1021.8, 1020.95, 1009.15, 1011.65, 1025.9, 1040.6, 1009.35, 1001.15, 986.1, 972.4, 983.75, 964.75, 943.9, 947.65, 949.95, 955.05, 959.3, 950.75, 955.45, 945.5, 948.0, 945.0, 920.65, 901.85, 915.5, 923.2, 909.4, 915.75, 906.0, 933.85, 945.45, 938.85, 951.85, 946.8, 951.05, 934.6, 937.75, 958.25, 949.2, 954.0, 944.5, 925.6, 911.35, 900.55, 887.55, 868.75, 833.25, 848.65, 845.8, 870.5, 871.4, 871.55, 871.1, 885.25, 898.5, 861.95, 868.4, 845.95, 852.9, 870.7, 854.85, 853.3, 847.75, 865.45, 867.55, 873.2, 867.6, 879.45, 876.2, 860.85, 873.35, 882.85, 871.5, 860.85, 837.35, 827.95, 830.85, 837.3, 825.05, 812.35, 792.0, 780.65, 765.3, 772.75, 772.2, 760.9, 759.15, 755.0, 741.2, 732.55, 757.0, 765.25, 761.3, 753.5, 719.4, 755.05, 782.6, 805.45, 788.85, 821.5, 833.5, 852.45, 825.75, 806.3, 810.25, 825.4, 836.3, 825.65, 821.55, 851.8, 837.6, 786.35, 770.6, 759.2, 797.2, 797.2, 807.7, 808.35, 788.2, 771.6, 767.0, 802.9, 829.25, 827.0, 849.65, 846.9, 866.9, 833.8, 819.9, 843.0, 838.7, 835.75, 846.5, 875.95, 873.6, 900.0, 877.55, 860.6, 871.8, 885.9, 879.95, 883.7, 864.95, 885.35, 876.7, 861.45, 848.1, 809.95, 808.1, 786.35, 795.2, 773.4, 773.15, 753.8, 783.8, 765.0, 778.25, 799.0, 812.05, 806.75, 809.8, 778.85, 755.7, 727.5, 742.3, 741.9, 750.15, 721.85, 735.3, 713.45, 748.0, 754.55, 746.4, 761.0, 753.05, 739.05, 711.9, 692.95, 706.55, 724.65, 716.15, 699.5, 717.55, 714.75, 706.25, 736.2, 748.25, 736.9, 731.8, 712.95, 742.05, 779.75, 785.65, 792.65, 771.55, 783.75, 790.95, 820.4, 794.75, 817.1, 831.2, 829.25, 838.3, 833.0, 844.55, 858.4, 853.85, 843.25, 849.25, 848.9, 836.65, 812.45, 818.2, 843.55, 833.4, 839.6, 819.95, 781.45, 796.5, 820.75, 810.95, 814.1, 813.45, 797.5, 776.55, 761.8, 773.8, 798.55, 819.95, 814.5, 796.65, 772.35, 755.1, 760.55, 768.05, 736.35, 744.05, 730.1, 731.05, 725.7, 724.65, 750.6, 740.55, 752.7, 747.0, 742.15, 742.65, 730.3, 743.3, 751.55, 748.1, 746.4, 748.6, 741.65, 730.85, 736.3, 735.1, 736.3, 745.05, 742.0, 739.95, 745.1, 743.55, 738.85, 726.45, 715.7, 708.35, 695.1, 694.15, 697.2, 681.3, 681.65, 676.1, 685.15, 688.55, 695.5, 691.1, 686.85, 695.3, 691.2, 701.65, 701.85, 706.25, 705.6, 685.25, 692.75, 702.55, 715.9, 721.25, 729.7, 722.25, 722.95, 716.3, 716.35, 726.75, 718.9, 737.45, 737.4, 718.7, 711.0, 716.2, 723.5, 722.5, 719.9, 737.85, 737.4, 736.35, 734.25, 738.7, 734.85, 731.8, 739.7, 725.65, 721.2, 718.5, 720.35, 719.35, 715.8, 727.8, 722.65, 719.0, 724.5, 719.25, 710.15, 721.0, 729.6, 743.6, 746.75, 739.55, 742.55, 786.05, 783.75, 790.8, 780.15, 782.1, 788.15, 799.55, 816.15, 816.1, 814.25, 807.8, 794.5, 782.1, 787.35, 790.2, 782.15, 783.4, 766.4, 763.05, 779.6, 771.15, 768.2, 791.75, 796.0, 792.05, 791.0, 798.1, 799.15, 841.75, 873.75, 855.6, 832.5, 850.3, 836.35, 839.75, 844.75, 829.85, 837.2, 833.5, 846.1, 852.9, 857.8, 819.4, 815.3, 816.0, 819.95, 818.9, 823.05, 812.45, 805.15, 811.3, 802.8, 815.6, 809.95, 810.4, 798.95, 811.2, 803.25, 805.5, 807.1, 807.95, 806.15, 805.45, 800.85, 796.6, 787.65, 786.65, 789.6, 784.6, 775.15, 774.3, 765.35, 771.2, 765.85, 773.75, 773.65, 786.8, 798.3, 793.7, 804.0, 824.7, 832.6, 843.9, 834.6, 828.3, 820.05, 832.6, 834.25, 839.25, 834.15, 831.4, 839.5, 837.85, 825.25, 820.7, 828.25, 817.7, 840.25, 839.55, 840.7, 847.95, 860.75, 861.85, 856.6, 850.65, 852.7, 850.05, 839.9, 847.55, 847.15, 861.35, 890.0, 900.2, 919.95, 920.05, 923.05, 915.55, 912.2, 897.55, 882.85, 899.4, 886.65, 894.0, 886.7, 874.75, 875.65, 872.9, 863.55, 868.4, 875.45, 879.0, 856.05, 845.8, 847.5, 848.4, 874.8, 859.2, 862.6, 854.35, 823.5, 831.3, 813.85, 810.4, 812.6, 826.35, 835.25, 836.5, 851.3, 844.5, 847.85, 845.0, 860.05, 843.65, 834.3, 825.0, 822.7, 812.6, 810.6, 809.25, 783.35, 772.9, 778.75, 793.9, 776.55, 767.55, 780.1, 789.4, 776.95, 779.4, 771.1, 775.35, 793.6, 804.95, 773.7, 781.6, 789.75, 803.7, 819.65, 792.4, 792.65, 788.2, 802.9, 800.5, 821.6, 828.15, 829.65, 818.1, 812.95, 814.1, 803.0, 798.15, 819.0, 840.65, 835.5, 831.15, 828.6, 818.75, 784.45, 786.45, 828.3, 843.4, 847.9, 836.8, 806.3, 787.9, 780.2, 802.0, 792.0, 783.6, 789.35, 785.6, 790.8, 789.8, 814.65, 825.55, 823.4, 831.7, 798.0, 792.85, 792.65, 803.8, 803.6, 830.0, 862.6, 885.7, 871.85, 850.85, 861.65, 880.35, 866.95, 874.8, 856.0, 870.05, 889.95, 896.85, 900.6, 914.95, 917.25, 923.7, 909.05, 909.55, 909.35, 890.75, 890.15, 885.7, 857.35, 872.05, 850.0, 857.35, 861.55, 841.05, 872.45, 865.65, 845.65, 852.9, 865.0, 826.7, 820.25, 825.35, 783.6, 806.1, 819.5, 822.6, 803.25, 811.8, 845.25, 853.85, 886.75, 831.0, 849.35, 862.55, 868.0, 890.35, 887.75, 875.4, 873.0, 861.75, 858.15, 871.95, 890.6, 881.9, 874.35, 874.6, 849.05, 838.25, 839.9, 822.4, 821.45, 846.95, 854.4, 844.05, 844.95, 860.9, 860.0, 863.35, 870.3, 867.8, 880.35, 905.65, 913.2, 904.2, 896.75, 884.5, 887.0, 890.75, 896.8, 898.15, 914.7, 908.25, 908.7, 909.7, 898.6, 885.9, 875.95, 863.9, 852.6, 844.4, 851.65, 874.75, 878.35, 862.5, 847.6, 843.75, 851.7, 841.25, 837.95, 847.95, 853.2, 855.3, 861.9, 856.05, 871.0, 866.7, 878.95, 884.65, 882.6, 870.2, 864.7, 845.05, 839.2, 859.95, 854.05, 895.15, 896.25, 898.05, 887.55, 878.85, 887.1, 895.2, 888.8, 875.0, 864.25, 854.95, 842.1, 849.05, 850.7, 858.15, 880.4, 881.5, 885.45, 886.15, 884.75, 869.75, 863.05, 872.45, 865.8, 867.65, 843.15, 844.85, 836.6, 825.0, 831.15, 820.9, 821.55, 817.25, 815.85, 814.45, 821.95, 805.25, 817.9, 807.4, 822.05, 812.3, 813.15, 812.25, 805.45, 811.5, 812.35, 810.55, 810.7, 799.95, 803.4, 812.95, 807.0, 822.45, 869.4, 885.45, 874.75, 871.35, 879.55, 886.15, 896.45, 903.0, 906.0, 885.8, 888.15, 905.4, 879.3, 891.2, 900.75, 916.15, 930.75, 942.6, 958.2, 952.65, 944.25, 944.15, 962.15, 969.05, 953.75, 949.1, 941.3, 959.1, 959.6, 967.75, 967.15, 948.25, 945.0, 940.3, 935.8, 928.1, 946.85, 963.35, 960.85, 960.3, 997.6, 1028.7, 1063.4, 1045.5, 1053.4, 1079.25, 1120.05, 1079.85, 1078.4, 1100.8, 1127.45, 1101.7, 1087.95, 1088.9, 1073.55, 1064.7, 1082.2, 1095.4, 1077.5, 1089.9, 1122.4, 1122.65, 1118.3, 1102.1, 1094.15, 1081.9, 1065.45, 1090.05, 1066.75, 1042.1, 1036.5, 1037.95, 1059.85, 1051.0, 1011.9, 1012.1, 1015.4, 1008.0, 1018.25, 1006.5, 1031.95, 1027.2, 995.05, 1000.95, 997.2, 966.5, 961.75, 972.2, 989.7, 982.5, 976.75, 997.65, 1030.95, 1033.25, 1041.05, 1021.85, 1005.6, 1006.15, 1006.45, 976.3, 985.0, 983.85, 988.15, 989.3, 980.5, 980.5, 984.4, 990.15, 1002.95, 1013.05, 1009.3, 998.35, 995.1, 998.3, 995.8, 993.35, 993.7, 998.7, 1014.1, 1023.8, 1031.25, 1025.9, 1025.65, 1037.7, 1033.0, 1015.85, 1019.4, 1015.6, 1002.75, 979.4, 988.65, 1004.9, 995.45, 991.55, 965.85, 965.05, 929.85, 933.25, 933.15, 945.7, 927.2, 926.15, 936.7, 954.65, 960.65, 957.9, 961.25, 931.0, 937.9, 935.05, 929.35, 945.65, 952.7, 941.35, 934.05, 951.6, 979.25, 1000.55, 1002.35, 991.3, 980.5, 969.35, 977.7, 974.05, 969.25, 969.15, 985.2, 988.65, 978.15, 980.75, 997.7, 985.35, 993.0, 987.05, 988.65, 991.6, 961.6, 962.65, 968.35, 958.55, 957.35, 944.6, 939.85, 932.55, 906.15, 882.35, 878.8, 864.6, 869.5, 880.1, 900.55, 904.9, 893.75, 884.15, 888.85, 897.0, 880.15, 891.15, 887.9, 885.55, 875.85, 836.1, 854.3, 842.05, 860.3, 850.3, 843.15, 834.95, 864.25, 869.7, 879.65, 903.05, 906.75, 883.75, 886.9, 889.05, 906.85, 929.4, 915.25, 908.05, 937.6, 928.8, 923.7, 909.9, 896.7, 882.0, 900.7, 909.15, 917.5, 902.8, 901.65, 902.05, 873.1, 851.45, 841.45, 843.15, 842.0, 855.55, 865.15, 864.25, 900.8, 887.85, 884.95, 869.65, 859.15, 855.95, 863.8, 850.1, 842.1, 856.85, 868.65, 856.15, 854.1, 841.95, 847.95, 844.9, 829.3, 811.7, 810.75, 826.0, 836.05, 824.65, 832.85, 865.9, 895.1, 904.35, 922.5, 923.65, 928.35, 926.85, 885.65, 876.45, 879.75, 877.05, 878.9, 875.3, 865.2, 850.0, 862.35, 890.95, 892.15, 880.25, 870.75, 889.7, 895.95, 868.7, 885.1, 878.2, 873.95, 891.55, 895.55, 908.85, 896.75, 903.85, 894.5, 884.6, 885.3, 875.8, 877.0, 902.25, 898.65, 896.35, 912.7, 907.75, 885.45, 883.7, 905.8, 877.15, 889.85, 901.75, 903.95, 931.2, 978.95, 996.65, 991.7, 995.0, 990.9, 1004.55, 1000.7, 990.85, 1000.1, 1005.65, 1010.85, 1007.2, 1015.55, 1003.25, 996.6, 987.4, 1001.8, 1007.0, 997.8, 1011.15, 1023.7, 1021.7, 1024.75, 1007.8, 1050.4, 1045.3, 1025.05, 1005.35, 1010.25, 996.0, 1000.15, 1001.85, 1002.45, 988.0, 987.75, 978.95, 984.35, 968.9, 966.0, 942.9, 933.35, 966.95, 950.7, 944.1, 954.6, 916.3, 907.95, 829.55, 848.15, 853.4, 869.15, 872.4, 855.9, 839.85, 849.55, 853.85, 835.9, 833.2, 848.6, 874.05, 867.0, 860.95, 863.85, 868.15, 872.55, 895.4, 878.1, 847.9, 850.35, 837.35, 834.25, 840.75, 862.3, 864.7, 888.0, 898.05, 913.8, 889.1, 887.35, 891.75, 893.6, 902.95, 903.7, 912.15, 963.5, 956.7, 955.0, 955.65, 942.85, 939.55, 936.95, 941.0, 947.7, 959.8, 967.2, 951.3, 936.6, 952.4, 961.25, 922.75, 928.45, 933.05, 934.3, 927.75, 912.0, 934.75, 946.85, 953.75, 967.7, 988.65, 979.05, 965.85, 965.9, 977.95, 977.2, 965.45, 952.05, 940.9, 915.9, 950.35, 952.95, 955.1, 975.9, 978.65, 1009.7, 991.9, 992.1, 987.3, 1003.8, 1000.55, 1010.4, 1015.4, 1004.5, 1014.6, 1015.35, 995.3, 1005.15, 1032.2, 1013.25, 1025.75, 1051.6, 1045.45, 1076.85, 1061.65, 1073.3, 1016.85, 1043.6, 1004.55, 984.2, 1004.15, 1003.95, 999.35, 1016.9, 1035.4, 1030.3, 1001.15, 981.4, 973.05, 970.9, 961.05, 957.5, 962.65, 924.55, 906.55, 946.75, 930.55, 952.6, 940.1, 944.35, 962.3, 949.65, 952.65, 944.1, 950.35, 966.65, 981.3, 1001.5, 1012.85, 1006.4, 1027.0, 1041.7, 1011.75, 1014.5, 1019.75, 1020.5, 1021.75, 1016.0, 1027.2, 1037.3, 1047.85, 1029.2, 1020.4, 1036.25, 1047.05, 1045.2]\n",
            "Epoch 1/50\n",
            "31/31 [==============================] - 14s 233ms/step - loss: 0.0295 - val_loss: 0.0113\n",
            "Epoch 2/50\n",
            "31/31 [==============================] - 6s 183ms/step - loss: 0.0075 - val_loss: 0.0081\n",
            "Epoch 3/50\n",
            "31/31 [==============================] - 5s 170ms/step - loss: 0.0064 - val_loss: 0.0069\n",
            "Epoch 4/50\n",
            "31/31 [==============================] - 7s 213ms/step - loss: 0.0056 - val_loss: 0.0061\n",
            "Epoch 5/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0050 - val_loss: 0.0050\n",
            "Epoch 6/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0045 - val_loss: 0.0046\n",
            "Epoch 7/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0041 - val_loss: 0.0051\n",
            "Epoch 8/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0038 - val_loss: 0.0053\n",
            "Epoch 9/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0034 - val_loss: 0.0047\n",
            "Epoch 10/50\n",
            "31/31 [==============================] - 6s 202ms/step - loss: 0.0034 - val_loss: 0.0065\n",
            "Epoch 11/50\n",
            "31/31 [==============================] - 6s 175ms/step - loss: 0.0037 - val_loss: 0.0040\n",
            "Epoch 12/50\n",
            "31/31 [==============================] - 5s 170ms/step - loss: 0.0035 - val_loss: 0.0057\n",
            "Epoch 13/50\n",
            "31/31 [==============================] - 6s 205ms/step - loss: 0.0033 - val_loss: 0.0038\n",
            "Epoch 14/50\n",
            "31/31 [==============================] - 5s 170ms/step - loss: 0.0032 - val_loss: 0.0052\n",
            "Epoch 15/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0031 - val_loss: 0.0038\n",
            "Epoch 16/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0034 - val_loss: 0.0038\n",
            "Epoch 17/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 0.0028 - val_loss: 0.0034\n",
            "Epoch 18/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0026 - val_loss: 0.0033\n",
            "Epoch 19/50\n",
            "31/31 [==============================] - 7s 215ms/step - loss: 0.0026 - val_loss: 0.0032\n",
            "Epoch 20/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0027 - val_loss: 0.0031\n",
            "Epoch 21/50\n",
            "31/31 [==============================] - 6s 194ms/step - loss: 0.0027 - val_loss: 0.0031\n",
            "Epoch 22/50\n",
            "31/31 [==============================] - 6s 182ms/step - loss: 0.0024 - val_loss: 0.0030\n",
            "Epoch 23/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0023 - val_loss: 0.0032\n",
            "Epoch 24/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0024 - val_loss: 0.0031\n",
            "Epoch 25/50\n",
            "31/31 [==============================] - 5s 171ms/step - loss: 0.0021 - val_loss: 0.0026\n",
            "Epoch 26/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 27/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 28/50\n",
            "31/31 [==============================] - 7s 215ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 29/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 30/50\n",
            "31/31 [==============================] - 6s 191ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 31/50\n",
            "31/31 [==============================] - 6s 182ms/step - loss: 0.0018 - val_loss: 0.0020\n",
            "Epoch 32/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0018 - val_loss: 0.0038\n",
            "Epoch 33/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0023 - val_loss: 0.0028\n",
            "Epoch 34/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 35/50\n",
            "31/31 [==============================] - 6s 210ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 36/50\n",
            "31/31 [==============================] - 5s 171ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 37/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 0.0015 - val_loss: 0.0019\n",
            "Epoch 38/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 39/50\n",
            "31/31 [==============================] - 6s 183ms/step - loss: 0.0015 - val_loss: 0.0020\n",
            "Epoch 40/50\n",
            "31/31 [==============================] - 6s 188ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 41/50\n",
            "31/31 [==============================] - 5s 168ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 42/50\n",
            "31/31 [==============================] - 6s 211ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 43/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 44/50\n",
            "31/31 [==============================] - 6s 209ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 45/50\n",
            "31/31 [==============================] - 5s 169ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 46/50\n",
            "31/31 [==============================] - 6s 208ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 47/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 48/50\n",
            "31/31 [==============================] - 6s 195ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 49/50\n",
            "31/31 [==============================] - 6s 182ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "31/31 [==============================] - 5s 167ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "31/31 [==============================] - 4s 61ms/step\n",
            "12/12 [==============================] - 1s 70ms/step\n",
            "     Company close price         predicted price\n",
            "0       BPCL     [904.3]   [[878.8542118787765]]\n",
            "1       GAIL     [356.4]  [[353.50252268165355]]\n",
            "2       NTPC    [128.85]  [[128.57849114090206]]\n",
            "3       ONGC     [214.1]  [[222.21683922335507]]\n",
            "4  POWERGRID     [139.1]  [[136.31156926453113]]\n",
            "5   RELIANCE    [1045.2]  [[1044.1167287826538]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d31c2acb9fe7>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  final_df = final_df.append({'Company': x, 'close price':ds[-1:], 'predicted price':output},ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        " #data = pd.read_csv('T.csv', sep=',')\n",
        "df = data[['Date', 'Symbol','Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "print(df.head())\n",
        "company = pd.DataFrame(df[\"Symbol\"].unique())\n",
        "company.columns =['Name']\n",
        "print(company)\n",
        "opn = []\n",
        "def training(name):\n",
        "  for index,row in df.iterrows():\n",
        "    if row[\"Symbol\"] == name:\n",
        "     opn.append(row[\"Close\"])\n",
        "  return opn\n",
        " #creating dataset in time series for LSTM model\n",
        "#X[100,120,140,160,180] : Y[200]\n",
        "def create_ds(dataset,step):\n",
        "    Xtrain, Ytrain = [], []\n",
        "    for i in range(len(dataset)-step-1):\n",
        "        a = dataset[i:(i+step), 0]\n",
        "        Xtrain.append(a)\n",
        "        Ytrain.append(dataset[i + step, 0])\n",
        "    return np.array(Xtrain), np.array(Ytrain)\n",
        "final_df = pd.DataFrame(columns=['Company', 'close price','predicted price'])\n",
        "\n",
        "for x in company[\"Name\"]:\n",
        "  ds = training(x)\n",
        "  print(ds)\n",
        "  #ds_100 = ds[-350:]\n",
        "    #Using MinMaxScaler for normalizing data between 0 & 1\n",
        "  normalizer = MinMaxScaler(feature_range=(0,1))\n",
        "  ds_scaled = normalizer.fit_transform(np.array(ds).reshape(-1,1))\n",
        "  train_size = int(len(ds_scaled)*0.70)\n",
        "  test_size = len(ds_scaled) - train_size\n",
        "    #Splitting data between train and test\n",
        "  ds_train, ds_test = ds_scaled[0:train_size,:], ds_scaled[train_size:len(ds_scaled),:1]\n",
        "\n",
        "    #Taking 100 days price as one record for training\n",
        "  time_stamp = 100\n",
        "  X_train, y_train = create_ds(ds_train,time_stamp)\n",
        "  X_test, y_test = create_ds(ds_test,time_stamp)\n",
        "    #Reshaping data to fit into LSTM model\n",
        "  X_train = X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
        "  X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
        "\n",
        "    #Creating LSTM model using keras\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))\n",
        "  model.add(LSTM(units=50,return_sequences=True))\n",
        "  model.add(LSTM(units=50))\n",
        "  model.add(Dense(units=1,activation='linear'))\n",
        "    #model.summary()\n",
        "\n",
        "    #Training model with adam optimizer and mean squared error loss function\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "  model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=50, batch_size=32)\n",
        "\n",
        "    #Predicitng on train and test data\n",
        "  train_predict = model.predict(X_train)\n",
        "  test_predict = model.predict(X_test)\n",
        "\n",
        "    #Inverse transform to get actual value\n",
        "  train_predict = normalizer.inverse_transform(train_predict)\n",
        "  test_predict = normalizer.inverse_transform(test_predict)\n",
        "\n",
        "  test = np.vstack((train_predict,test_predict))\n",
        "\n",
        "    #Getting the last 100 days records\n",
        "  fut_inp = ds_test[-100:]\n",
        "  fut_inp = fut_inp.reshape(1,-1)\n",
        "  tmp_inp = list(fut_inp)\n",
        "\n",
        "    #Creating list of the last 100 data\n",
        "  tmp_inp = tmp_inp[0].tolist()\n",
        "\n",
        "    #Predicting next 30 days price suing the current data\n",
        "    #It will predict in sliding window manner (algorithm) with stride 1\n",
        "  lst_output=[]\n",
        "\n",
        "  n_steps=100\n",
        "  i=0\n",
        "  while(i<1):\n",
        "\n",
        "      if(len(tmp_inp)>100):\n",
        "          fut_inp = np.array(tmp_inp[1:])\n",
        "          fut_inp=fut_inp.reshape(1,-1)\n",
        "          fut_inp = fut_inp.reshape((1, n_steps, 1))\n",
        "          yhat = model.predict(fut_inp, verbose=0)\n",
        "          tmp_inp.extend(yhat[0].tolist())\n",
        "          tmp_inp = tmp_inp[1:]\n",
        "          lst_output.extend(yhat.tolist())\n",
        "\n",
        "          i=i+1\n",
        "      else:\n",
        "          fut_inp = fut_inp.reshape((1, n_steps,1))\n",
        "          yhat = model.predict(fut_inp, verbose=0)\n",
        "          tmp_inp.extend(yhat[0].tolist())\n",
        "          lst_output.extend(yhat.tolist())\n",
        "\n",
        "          i=i+1\n",
        "\n",
        "  output= normalizer.inverse_transform(lst_output[-1:])\n",
        "  final_df = final_df.append({'Company': x, 'close price':ds[-1:], 'predicted price':output},ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "  ds.clear()\n",
        "\n",
        "\n",
        "\n",
        "print(final_df)"
      ]
    }
  ]
}